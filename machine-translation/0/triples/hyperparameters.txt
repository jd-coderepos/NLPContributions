(Contribution||has||Hyperparameters)
(Hyperparameters||for||RNN Encoder-Decoder)
(RNN Encoder-Decoder||activation function||hyperbolic tangent function)
(RNN Encoder-Decoder||trained for||approximately three days)
(RNN Encoder-Decoder||implemented||computation from the hidden state in the decoder to the output)
(computation from the hidden state in the decoder to the output||as a||deep neural network (Pascanu et al., 2014))
(deep neural network (Pascanu et al., 2014)||with||single intermediate layer)
(single intermediate layer||having||500 maxout units)
(500 maxout units||pooling||2 inputs)
(RNN Encoder-Decoder||initialized||weight parameters)
(weight parameters||by||sampling from an isotropic zero-mean (white) Gaussian distribution)
(sampling from an isotropic zero-mean (white) Gaussian distribution||with||standard deviation fixed to 0.01)
(RNN Encoder-Decoder||used||rank-100 matrices)
(RNN Encoder-Decoder||hidden units||1000)
(1000||with||proposed gates at the encoder and at the decoder)
(RNN Encoder-Decoder||Adadelta and stochastic gradient descent||epsilon = 10-6 and rho = 0.95 (Zeiler, 2012))
