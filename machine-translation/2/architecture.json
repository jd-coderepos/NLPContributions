{
  "has" : {
    "Model Architecture" : {
      "has" : {
        "Encoder and Decoder Stacks" : {
          "has" : {
            "Encoder" : {
              "composed of" : {
                "stack of N = 6 identical layers" : {
                  "has" : {
                    "two sub-layers" : {
                      "first" : "multi-head self-attention mechanism",
                      "second" : "simple, position-wise fully connected feed-forward network"
                    }
                  }
                },
                "from sentence" : "The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network."
              },
              "employ" : {
                "residual connection" : {
                  "around" : {
                    "two sub-layers" : {
                      "followed by" : "layer normalization"
                    }
                  }
                },
                "from sentence" : "We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]."
              } 
            },
            "Decoder" : {
              "composed of" : {
                "stack of N = 6 identical layers" : {
                  "has" : {
                    "third sub-layer" : {
                      "performs" : {
                        "multi-head attention" : {
                          "over" : "output of the encoder stack"
                        }
                      }
                    }
                  }
                },
                "from sentence" : "The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack."                
              },
              "employ" : {
                "residual connections" : {
                  "around" : "each of the sub-layers",
                  "followed by" : "layer normalization"
                },
                "from sentence" : "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
              },
              "modify" : {
                "self-attention sub-layer" : {
                  "in" : {
                    "decoder stack" : {
                      "prevent" : "positions from attending to subsequent positions"
                    }
                  }
                },
                "from sentence" : "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions."
              }
            }
          }
        },
        "Attention" : {
          "described as" : ["mapping a query and a set of key-value pairs to an output", "weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key", {"from sentence" : "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key."}],
          "has" : ["Scaled Dot-Product Attention",
            "Multi-Head Attention"]
        },
        "Position-wise Feed-Forward Networks" : {
          "contains" : {
            "fully connected feed-forward network" : {
              "applied to" : {
                "each position separately and identically" : {
                  "consists" : {
                    "two linear transformations" : {
                      "with" : "ReLU activation"
                    }
                  }
                }
              }
            },
            "from sentence" : "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."
          }
        },
        "Embeddings and Softmax" : {
          "use" : {
            "learned embeddings" : {
              "convert" : "input tokens and output tokens to vectors of dimension dmodel",
              "from sentence" : "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel."
            },
            "usual learned linear transformation and softmax function" : {
              "convert" : "decoder output to predicted next-token probabilities",
              "from sentence" : "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities."
            }
          }
        },
        "Positional Encoding" : {
          "inject" : {
            "some information about the relative or absolute position" : {
              "of" : "tokens in the sequence"
            },
            "from sentence" : "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence."
          },
          "add to" : {
            "input embeddings" : {
              "at" : "bottoms of the encoder and decoder stacks"
            },
            "from sentence" : "To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks"
          }
        }
      }
    }
  }
}