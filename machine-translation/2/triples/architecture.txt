(Contribution||has||Model Architecture)
(Model Architecture||has||Encoder and Decoder Stacks)
(Encoder and Decoder Stacks||has||Encoder)
(Encoder||employ||residual connection)
(residual connection||around||two sub-layers)
(two sub-layers||followed by||layer normalization)
(Encoder||composed of||stack of N = 6 identical layers)
(stack of N = 6 identical layers||has||two sub-layers)
(two sub-layers||first||multi-head self-attention mechanism)
(two sub-layers||second||simple, position-wise fully connected feed-forward network)
(Encoder and Decoder Stacks||has||Decoder)
(Decoder||employ||residual connections)
(residual connections||followed by||layer normalization)
(residual connections||around||each of the sub-layers)
(Decoder||modify||self-attention sub-layer)
(self-attention sub-layer||in||decoder stack)
(decoder stack||prevent||positions from attending to subsequent positions)
(Decoder||composed of||stack of N = 6 identical layers)
(stack of N = 6 identical layers||has||third sub-layer)
(third sub-layer||performs||multi-head attention)
(multi-head attention||over||output of the encoder stack)
(Model Architecture||has||Position-wise Feed-Forward Networks)
(Position-wise Feed-Forward Networks||contains||fully connected feed-forward network)
(fully connected feed-forward network||applied to||each position separately and identically)
(each position separately and identically||consists||two linear transformations)
(two linear transformations||with||ReLU activation)
(Model Architecture||has||Attention)
(Attention||described as||mapping a query and a set of key-value pairs to an output)
(Attention||described as||weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key)
(Attention||has||Scaled Dot-Product Attention)
(Attention||has||Multi-Head Attention)
(Model Architecture||has||Positional Encoding)
(Positional Encoding||inject||some information about the relative or absolute position)
(some information about the relative or absolute position||of||tokens in the sequence)
(Positional Encoding||add to||input embeddings)
(input embeddings||at||bottoms of the encoder and decoder stacks)
(Model Architecture||has||Embeddings and Softmax)
(Embeddings and Softmax||use||usual learned linear transformation and softmax function)
(usual learned linear transformation and softmax function||convert||decoder output to predicted next-token probabilities)
(Embeddings and Softmax||use||learned embeddings)
(learned embeddings||convert||input tokens and output tokens to vectors of dimension dmodel)
