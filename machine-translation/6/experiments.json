{
  "has" : {
    "Experiments" : {
      "tasks" : {
        "Word Similarity" : {
          "has description" : ["evaluates whether the most similar words of a given word in the embedding space are consistent with the ground-truth", {"from sentence" : "Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity: it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground-truth, in terms of Spearman’s rank correlation."}],
          "baseline" : ["skip-gram model", {"from sentence" : "We use the skip-gram model as our baseline model [30], and train the embeddings using Enwik9."}],
          "three datasets" : ["RG65", "WS", "RW", {"from sentence" : "We test the baseline and our method on three datasets: RG65, WS and RW."}]
        },
        "Language Modeling" : {
          "has description" : ["predict the next word conditioned on previous words", {"from sentence" : "Language Modeling is a basic task in natural language processing. The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity."}],
          "baselines" : ["AWD-LSTM model", "AWD-LSTM-MoS model", {"from sentence" : "We choose two recent works as our baselines: the AWD-LSTM model [27] and the AWD-LSTM-MoS model, [45] which achieves state-of-the-art performance."}],
          "datasets" : ["Penn Treebank (PTB)", "WikiText-2 (WT2)", {"from sentence" : "We do experiments on two widely used datasets [27, 28, 45], Penn Treebank (PTB) [29] and WikiText-2 (WT2) [28]."}]
        },
        "Machine Translation" : {
          "two datasets" : ["WMT14 English-German", "IWSLT14 German-English datasets", {"from sentence" : "We choose two datasets: WMT14 English-German and IWSLT14 German-English datasets, which are evaluated in terms of BLEU score."}],
          "baseline" : ["Transformer", {"from sentence" : "We use Transformer [42] as the baseline model, which achieves state-of-the-art accuracy on multiple translation datasets."}]
        },
        "Text Classification" : {
          "dataset" : ["AG's news corpus (AGs)", "IMDB movie review dataset (IMDB)", "20 Newsgroups (20NG)", {"from sentence" : "Following the setting in [22], we implement a Recurrent CNN-based model and test it on AG's news corpus (AGs), IMDB movie review dataset (IMDB) and 20 Newsgroups (20NG)."}],
          "baseline" : ["Recurrent CNN-based model", {"from sentence" : "Following the setting in [22], we implement a Recurrent CNN-based model11 and test it on AG’s news corpus (AGs), IMDB movie review dataset (IMDB) and 20 Newsgroups (20NG)."}]
        }
      },
      "hyperparameters" : {
        "discriminator" : {
          "use" : {
            "logistic regression" : {
              "for" : "language modeling and machine translation"
            },
            "shallow neural network" : {
              "with" : "one hidden layer"
            }
          },
          "from sentence" : "For language modeling and machine translation tasks, we use logistic regression as the discriminator. For other tasks, we find using a shallow neural network with one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size."
        }
      }
    }
  }
}