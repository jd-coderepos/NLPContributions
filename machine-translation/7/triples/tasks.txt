(Contribution||has||tasks)
(tasks||MULTILINGUAL MACHINE TRANSLATION||Results)
(Results||significantly beats||multilingual GNMT model)
(multilingual GNMT model||by||the MoE model)
(the MoE model||on||BLEU score)
(the MoE model||on||11 of the 12 language pairs (by as much as 5.84 points))
(the MoE model||on||8 of 12 language pairs)
(Results||achieves||19% lower perplexity on the dev set)
(19% lower perplexity on the dev set||by||The MoE model)
(The MoE model||than||multilingual GNMT model)
(tasks||MULTILINGUAL MACHINE TRANSLATION||repeat)
(repeat||this experiment||single MoE-augmented model)
(tasks||has||MACHINE TRANSLATION (SINGLE LANGUAGE PAIR))
(MACHINE TRANSLATION (SINGLE LANGUAGE PAIR)||Results||BLEU scores of 40.56 and 26.03)
(BLEU scores of 40.56 and 26.03||on||WMT'14 En->Fr and En->De benchmarks)
(MACHINE TRANSLATION (SINGLE LANGUAGE PAIR)||Results||1.01 higher test BLEU score)
(1.01 higher test BLEU score||on||Google Production dataset)
(MACHINE TRANSLATION (SINGLE LANGUAGE PAIR)||Model Architecture||GNMT model)
(GNMT model||modified version||inserted)
(inserted||MoE layers||in)
(in||both||encoder (between layers 2 and 3))
(in||both||decoder (between layers 1 and 2))
(inserted||MoE layers||each layer)
(each layer||contained up to||2048 experts)
(2048 experts||with||two million parameters)
(GNMT model||modified version||decreased)
(decreased||LSTM layers in the encoder and decoder||from)
(from||9 and 8 to||3 and 2)
