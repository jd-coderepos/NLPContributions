{
  "has" : {
    "Approach" : {
      "differ" : ["traditional word type embeddings", {"from sentence" : "Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence."}],
      "assigned" : {
        "each token" : {
          "representation" : "function of the entire input sentence"
        },
        "from sentence" : "Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence."
      },
      "use" : {
        "vectors" : {
          "derived from" : "bidirectional LSTM",
          "trained with" : {
            "coupled language model (LM) objective" :{
              "on" : "large text corpus"
            }
          }
        },
        "from sentence" : "We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus."
      },
      "called" : ["ElMo (Embeddings from Language Models) representations", {"from sentence" : "For this reason, we call them ELMo (Embeddings from Language Models) representations."}],
      "learn" : {
        "linear combination of the vectors" : {
          "stacked above" : {
            "each input word" : {
              "for" : {
                "each end task" : {
                  "markedly improves" : {
                    "performance" : {
                      "over" : "using the top LSTM layer"
                    }
                  }
                }
              }
            }
          }
        },
        "from sentence" : "More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer."
      }
    }
  }
}