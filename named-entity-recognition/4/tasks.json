{
  "has" : {
    "diverse set of six benchmark NLP tasks" : {
      "Question answering" : {
        "on" : {
          "Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016)" : {
            "test set F1" : {
              "improved by 4.7%" : {
                "from" : {
					"81.1%" : "to 85.8%"				
				}
              }
            }
          },
          "from sentence" : "Question answering The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) contains 100K+ crowd sourced question-answer pairs where the answer is a span in a given Wikipedia paragraph. After adding ELMo to the baseline model, test set F1 improved by 4.7% from 81.1% to 85.8%, a 24.9% relative error reduction over the baseline, and improving the overall single model state-of-the-art by 1.4%."
        }
      },
      "Textual entailment" : {
        "on" : {
          "Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015)" : {
            "improves" : {
              "accuracy" : {
                "by" : {
                  "average of 0.7%" : {
                    "across" : {
						"five" : "random seeds"
					}
                  }
                }
              }
            }
          }
        },
        "from sentence" : "The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) provides approximately 550K hypothesis/premise pairs. Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds."
      },
      "Semantic role labeling" : {
        "on" : {
          "OntoNotes benchmark (Pradhan et al., 2013)" : {
            "test set F1" : {
              "jumped 3.2%" : {
                "from" : {
					"81.4%" : "to 84.6%"
				}
              }
            }
          }
        },
        "from sentence" : "As shown in Table 1, when adding ELMo to a re-implementation of He et al. (2017) the single model test set F1 jumped 3.2% from 81.4% to 84.6% â€“ a new state-of-the-art on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the previous best ensemble result by 1.2%."
      },
      "Coreference resolution" : {
        "on" : {
          "OntoNotes coreference annotations" : {
            "improved" : {
              "average F1" : {
				"by 3.2%" : {
					"from" : "67.2 to 70.4"
				}
              }
            }
          }
        },
        "from sentence" : "In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task (Pradhan et al., 2012), adding ELMo improved the average F1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the previous best ensemble result by 1.6% F1." 
      },
      "Named entity extraction" : {
        "on" : {
          "CoNLL 2003 NER task (Sang and Meulder, 2003)" : {
            "achieves" : {
				"92.22% F1" : "averaged over five runs"
			}
          }
        },
        "from sentence" : "Named entity extraction The CoNLL 2003 NER task (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F1 averaged over five runs."
      },
      "Sentiment analysis" : {
        "on" : {
          "Stanford Sentiment Treebank (SST-5; Socher et al., 2013)" : {
            "results in" : {
				"improvement" : "1.0% absolute accuracy"
			}
          }
        }
      }
    }
  }
}