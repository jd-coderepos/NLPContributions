(Contribution||has||diverse set of six benchmark NLP tasks)
(diverse set of six benchmark NLP tasks||Coreference resolution||on)
(on||OntoNotes coreference annotations||improved)
(improved||average F1||by 3.2%)
(by 3.2%||from||67.2 to 70.4)
(diverse set of six benchmark NLP tasks||Sentiment analysis||on)
(on||Stanford Sentiment Treebank (SST-5; Socher et al., 2013)||results in)
(results in||improvement||1.0% absolute accuracy)
(diverse set of six benchmark NLP tasks||Textual entailment||on)
(on||Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015)||improves)
(improves||accuracy||by)
(by||average of 0.7%||across)
(across||five||random seeds)
(diverse set of six benchmark NLP tasks||Named entity extraction||on)
(on||CoNLL 2003 NER task (Sang and Meulder, 2003)||achieves)
(achieves||92.22% F1||averaged over five runs)
(diverse set of six benchmark NLP tasks||Question answering||on)
(on||Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016)||test set F1)
(test set F1||improved by 4.7%||from)
(from||81.1%||to 85.8%)
(diverse set of six benchmark NLP tasks||Semantic role labeling||on)
(on||OntoNotes benchmark (Pradhan et al., 2013)||test set F1)
(test set F1||jumped 3.2%||from)
(from||81.4%||to 84.6%)
