{
  "has" : {
    "11 NLP tasks" : {
      "GLUE" : [
        ["General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a)",
        {"from sentence" : "The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks."}],
        {
          "Hyperparameters" : {
            "batch size" : "32",
            "fine-tune" : "3 epochs",
            "learning rate" : "5e-5, 4e-5, 3e-5, and 2e-5",
            "from sentence" : "We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set."
          }
        },
        {
          "Results" : {
            "outperform all systems on all tasks" : {
              "BERTBASE and BERTLARGE outperform all systems on all tasks" : {
                "by" : {
                  "substantial margin" : {
                    "obtaining" : "4.5% and 7.0% respective average accuracy improvement"
                  }
                }
              },
              "from sentence" : "Both BERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art."
            },
            "significantly outperforms" : ["BERTLARGE significantly outperforms BERTBASE across all tasks", {"from sentence" : "We find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data."}]
          }
        }
      ],
      "SQuAD v1.1" : [
        ["Stanford Question Answering Dataset (SQuAD v1.1)",
        {"from sentence" : "The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs (Rajpurkar et al., 2016)."}],
        {
          "Hyperparameters" : {
            "batch size" : "32",
            "fine-tune" : "3 epochs",
            "learning rate" : "5e-5",
            "from sentence" : "We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32."
          }
        },
        {
          "Results" : {
            "outperforms" : {
              "Our best performing system outperforms the top leaderboard system" : {
                "by" : {
                  "+1.5 F1" : {
                    "in" : "ensembling"
                  },
                  "+1.3 F1" : {
                    "as" : "single system"
                  }
                }
              },
              "from sentence" : "Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system."
            }
          }
        }
      ],
      "SQuAD v2.0" : [
        {
          "Hyperparameters" : {
            "batch size" : "48",
            "fine-tune" : "2 epochs",
            "learning rate" : "5e-5",
            "from sentence" : "We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48."
          }
        },
        {
          "Results" : {
            "improvement" : {
              "+5.1 F1 improvement" : {
                "over" : "previous best system"
              },
              "from sentence" : "We observe a +5.1 F1 improvement over the previous best system."
            }
          } 
        }
      ],
      "SWAG" : [
        ["Situations With Adversarial Generations (SWAG) dataset", {"from sentence" : "The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018)."}],
        {
          "Hyperparameters" : {
            "batch size" : "16",
            "fine-tune" : "3 epochs",
            "learning rate" : "2e-5",
            "from sentence" : "We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16."
          }
        },
        {
          "Results" : {
            "BERTLARGE outperforms" : {
              "authors' baseline ESIM+ELMo system" : {
                "by" : "+27.1%"
              },
              "OpenAI GPT" : {
                "by": "8.3%"
              },
              "from sentence" : "BERTLARGE outperforms the authorsâ€™ baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%."
            }
          } 
        }        
      ]
    }
  }
}