(Contribution||has||Approach)
(Approach||proposing||BERT: Bidirectional Encoder Representations from Transformers)
(Approach||called||BERT)
(BERT||alleviates||previously mentioned unidirectionality constraint)
(previously mentioned unidirectionality constraint||using||"masked language model" (MLM) pre-training objective)
("masked language model" (MLM) pre-training objective||inspired by||Cloze task (Taylore, 1953))
("masked language model" (MLM) pre-training objective||randomly masks||some of the tokens from the input)
(some of the tokens from the input||objective||predict the original vocabulary id of the masked word based only on its context)
("masked language model" (MLM) pre-training objective||enables||representation to fuse the left and the right context)
(representation to fuse the left and the right context||to pretrain||deep bidirectional Transformer)
(BERT||use||"next sentence prediction" task)
("next sentence prediction" task||jointly pretrains||text-pair representations)
