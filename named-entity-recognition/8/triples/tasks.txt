(Contribution||has||11 NLP tasks)
(11 NLP tasks||GLUE||General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a))
(11 NLP tasks||GLUE||Hyperparameters)
(Hyperparameters||learning rate||5e-5, 4e-5, 3e-5, and 2e-5)
(Hyperparameters||batch size||32)
(Hyperparameters||fine-tune||3 epochs)
(11 NLP tasks||GLUE||Results)
(Results||outperform all systems on all tasks||BERTBASE and BERTLARGE outperform all systems on all tasks)
(BERTBASE and BERTLARGE outperform all systems on all tasks||by||substantial margin)
(substantial margin||obtaining||4.5% and 7.0% respective average accuracy improvement)
(Results||significantly outperforms||BERTLARGE significantly outperforms BERTBASE across all tasks)
(11 NLP tasks||SQuAD v1.1||Stanford Question Answering Dataset (SQuAD v1.1))
(11 NLP tasks||SQuAD v1.1||Hyperparameters)
(Hyperparameters||learning rate||5e-5)
(Hyperparameters||batch size||32)
(Hyperparameters||fine-tune||3 epochs)
(11 NLP tasks||SQuAD v1.1||Results)
(Results||outperforms||Our best performing system outperforms the top leaderboard system)
(Our best performing system outperforms the top leaderboard system||by||+1.5 F1)
(+1.5 F1||in||ensembling)
(Our best performing system outperforms the top leaderboard system||by||+1.3 F1)
(+1.3 F1||as||single system)
(11 NLP tasks||SQuAD v2.0||Hyperparameters)
(Hyperparameters||learning rate||5e-5)
(Hyperparameters||batch size||48)
(Hyperparameters||fine-tune||2 epochs)
(11 NLP tasks||SQuAD v2.0||Results)
(Results||improvement||+5.1 F1 improvement)
(+5.1 F1 improvement||over||previous best system)
(11 NLP tasks||SWAG||Situations With Adversarial Generations (SWAG) dataset)
(11 NLP tasks||SWAG||Hyperparameters)
(Hyperparameters||learning rate||2e-5)
(Hyperparameters||batch size||16)
(Hyperparameters||fine-tune||3 epochs)
(11 NLP tasks||SWAG||Results)
(Results||BERTLARGE outperforms||OpenAI GPT)
(OpenAI GPT||by||8.3%)
(Results||BERTLARGE outperforms||authors' baseline ESIM+ELMo system)
(authors' baseline ESIM+ELMo system||by||+27.1%)
