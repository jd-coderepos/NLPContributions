(Contribution||has||Approach)
(Approach||test||pre-training strategies)
(pre-training strategies||analyze||effect of each corpus on pre-training)
(pre-training strategies||on||general domain corpora and biomedical corpora)
(general domain corpora and biomedical corpora||with||different combinations and sizes)
(Approach||introduce||BioBERT)
(BioBERT||is||pre-trained language representation model)
(pre-trained language representation model||for||biomedical domain)
(Approach||pre-trained||BioBERT)
(BioBERT||on||biomedical domain corpora (PubMed abstracts and PMC full-text articles))
(Approach||fine-tuned and evaluated||BioBERT)
(BioBERT||on||three popular biomedical text mining tasks (NER, RE and QA))
(Approach||initialize||BioBERT)
(BioBERT||with||weights from BERT)
