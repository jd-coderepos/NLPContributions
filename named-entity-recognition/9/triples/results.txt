(Contribution||has||Results)
(Results||Hyperparamaters||NVIDIA V100 (32GB) GPUs)
(NVIDIA V100 (32GB) GPUs||used||eight)
(eight||for||pre-training)
(Results||Hyperparamaters||NVIDIA Titan Xp (12GB) GPU)
(NVIDIA Titan Xp (12GB) GPU||used||single)
(single||to||fine-tune BioBERT)
(Results||Hyperparamaters||BioBERT v1.0 (+ PubMed + PMC))
(BioBERT v1.0 (+ PubMed + PMC)||trained for||470K steps)
(Results||Hyperparamaters||mini-batch size)
(mini-batch size||set to||192)
(Results||Hyperparamaters||fine-tuning)
(fine-tuning||learning rate||5e-5, 3e-5 or 1e-5)
(fine-tuning||batch size||10, 16, 32, or 64)
(Results||Hyperparamaters||batch size and learning rate scheduling)
(batch size and learning rate scheduling||same as those||pre-training BERT)
(Results||Hyperparamaters||Naver Smart Machine Learning (NSML) (Sung et al., 2017)
(Naver Smart Machine Learning (NSML) (Sung et al., 2017||using||pre-trained BioBERT)
(Results||Hyperparamaters||PMC)
(PMC||found||270K)
(270K||optimal||pre-training steps)
(Results||Hyperparamaters||maximum sequence length)
(maximum sequence length||fixed||512)
(Results||Hyperparamaters||PubMed)
(PubMed||found||200K)
(200K||optimal||pre-training steps)
(Results||Experimental results||RE results)
(RE results||on||2 out of 3 biomedical datasets)
(2 out of 3 biomedical datasets||achieved||highest F1 scores)
(RE results||on||CHEMPROT dataset)
(CHEMPROT dataset||achieved better performance||than the state-of-the-art model)
(Results||Experimental results||six out of nine datasets)
(six out of nine datasets||outperformed||state-of-the-art models)
(Results||Experimental results||QA results)
(QA results||significantly outperformed||BERT and the state-of-the-art models)
(QA results||on||all the biomedical QA datasets)
(all the biomedical QA datasets||achieved||new state-of-the-art performance in terms of MRR)
(Results||Experimental results||all datasets)
(all datasets||achieves||higher scores than BERT)
