unsupervised NMT	1
Language Modeling	2
words and entity types	1
BERTLARGE significantly outperforms BERTBASE across all tasks	1
efficient graph convolution operations	1
more complex pre-processing techniques	1
local region K2 x C	1
phrase-based SMT	1
GCN model outperforms all dependency-based models	1
10-5	1
10-3	1
used to initialize word vectors for all three datasets	1
all three proposed dynamic routing strategies	1
470K steps	1
core NLP tasks	1
RaSoR	1
+1.92 F1	1
both ensembles of averaged models	1
dependency structure over the input sentence	1
Binary phrase level sentiment classification	1
democratize large-scale NLP and information extraction while minimizing our environmental footprint	1
each word vector	1
strict evaluation metric	1
batch size and learning rate scheduling	1
bigrams	1
Fully Connected Capsule Layer	1
Word pairs from the psychology literature on implicit association tests (IAT) that are used to characterize model bias	1
weak supervision	1
https://github.com/datquocnguyen/jointRE	1
3	3
model with a better quality	1
4	6
EBMNLP (Nye et al., 2018)	1
5	1
position information of a word in the entity	1
8	1
extension of graph convolutional networks that is tailored for relation extraction, which pools information over arbitrary dependency structures efficiently in parallel	1
Path-centric Pruning	1
bidirectional sequential LSTM (BiLSTM) layer	1
English-to-French translation	1
loss-free compression rate	1
OntoNotes	2
Removing label embeddings did not affect the entity detection performance, but this degraded the recall in relation classification.	1
n-gram functionality	1
simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely	1
Neural network approaches to Named-Entity Recognition	1
than the state-of-the-art model	1
1000	4
Stack-LSTM model also outperforms all previous models	1
word-level systems	1
result	1
orthographic sensitivity	1
semi-supervised approach	1
Extractive Question Ansering	1
a pre-trained biomedical language representation model	1
other CNN baseline methods with much deeper architectures	1
Matching two potentially heterogenous language objects	1
test set F1	2
SemEval 2018 Task 7	1
joint vector space	1
computer science tasks	1
single machine	2
movie review dataset	1
all Reverb facts	1
corpus of 840bn words	1
+5.1 F1 improvement	1
building a deep topology	1
semantic matching vector	2
overall improvement of ~1%	1
MCTest	1
novel recurrent neural model	1
best results	2
recurrent layer (specifically an LSTM (Hochreiter and Schmidhuber, 1997))	1
feature generator	1
results in	1
single recurrent unit	1
batch of sequences	1
multi-GPU setup	1
scalable and effective solution	1
DeepMatch	1
CoVe	1
local-3 function	1
GloVe embeddings	3
our ACNN framework	1
10k steps	1
DRNNs	1
1.31 bits/character on the test set	1
range [-0.01, 0.01]	1
take the whole sentence as input (with word embedding aligned sequentially), and use an MLP to obtain the score of coherence	1
input embeddings	2
neural architectures for NER	1
11 NLP tasks	1
TensorFlow (Abadi et al., 2018)	1
fixing vectors	1
improves performance	1
L2 regularization parameter	1
endpoint prediction model	1
Dialog	1
neural attention-based multiple context fixing attachment (MCFA)	1
weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key	1
pre-trained 100-D Glove 6B vectors	1
whole sentence as single state	1
new type of general purpose neural network component	1
Bi-LSTM with local decoding and one with CRF decoding	1
an absolute gain of 1.68 and 0.96 points respectively	1
instantiated parameters	1
any neural NER system	1
In this paper, we present a novel end-to-end neural model for joint entity and relation extraction.	1
When predicting multiple relations in one-pass, we have 0.9% drop on Macro-F1	1
both fine-tuning and feature-based approaches	1
VC-CNN	1
chain structured graphical model	1
reduce the number of parameters used in word embeddings without hurting the model performance	1
600	1
version of GNMT that uses word-pieces	1
each trigger through time	1
LSTM-LSTM model is better than LSTM-CRF model	1
less sensitive to changes in the tree structures provided	1
win-3 with 4-gram filters	1
jointly on different data sources via multitasking	1
Constrained English-Estonian and Estonian-English NMT systems	1
Ablation Study	1
more than 2.0 BLEU	1
S-ACNN significantly outperforms S-CNN	1
natural language understanding	4
each model twice	1
our model outperforms the previous shortest dependency path-based model (SDP-LSTM)	1
bag of n-grams	1
Convolutional Capsule Layer	1
boundaries of recurrent language models and encoder-decoder architectures	1
CoNLL-2003 English NER	1
regularize our network	1
our ACNN model	1
language understanding	1
morphological information	1
We also report results for the DREC dataset, with two different evaluation settings. In the boundaries evaluation, we achieve ~3% improvement for both tasks.	1
ADE dataset	1
pre-trained 300-D Glove 840B vectors	2
input words and the predicted NER labels	1
more general framework	1
feed forward neural networks	2
with	2
a linear mapping layer	1
neural language model (LM), pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence (hereafter an LM embedding) and use it in the supervised sequence tagging model	1
Impact of LM quality	1
Sentiment of sentences mined from customer reviews	1
d=64	1
standard settings (Kingma and Ba, 2014)	1
comparable performance	1
STS Benchmark	1
each for one language	1
scientific text	1
Bi-LSTM-CRF	1
feature maps	2
back-translated data	1
our model improves upon other dependency-based models in both precision and recall	1
IWSLT14 German-English task	1
RNNsearch	2
gain mainly comes from improved recall	1
exact match score of 61.1%	1
+27.1%	1
jointly learn to extract entities and relations	1
named entity recognition (NER) component	1
jointly	2
We model the relation extraction task as a multi-label head selection problem (Bekoulis et al., 2018b; Zhang et al., 2017).	1
Word-by-word translation (WBW)	1
d is 800	1
SQUAD training set	1
deep convolutional neural networks with 9/17/29 layers	1
Fast and Accurate Entity Recognition	1
sentence embeddings	3
2048 experts	1
hypothesis to sentences in the text viewed as single, self-contained thoughts	1
SMT	1
[0.9, 0.95, 1.0]	1
predicting the binary label	1
Our MMA-NSE attention model	1
different window sizes h = 3, 4, 5	1
The basic layers of the model, shown in Fig. 1, are: (i) embedding layer, (ii) bidirectional sequential LSTM (BiLSTM) layer, (iii) CRF layer and the (iv) sigmoid scoring layer.	1
models that use extensive sets of hand-crafted features	1
further analysis is needed	1
shuffled mini-batches	1
a sentence vector (e.g. Korean)	1
ADAM (Kingma & Ba, 2015) optimizer	1
SciCite (Cohan et al., 2019)	1
pooled features directly to the fully connected softmax layers	1
our AT model beats the baseline F1	1
calculates attention weights with respect to the entity pairs, word positions relative to these pairs, and their latent types obtained by LET	1
16 x 32 coding scheme	1
lemmatization and multiword grouping	1
noise	1
learned embeddings	1
document-level context	1
Our model first detects entities and then extracts relations between the detected entities using a single incrementally-decoded NN structure, and the NN parameters are jointly updated using both entity and relation labels.	1
baseline performance	1
transfer learning via sentence embeddings	1
semantic perspective	1
fast-forward connections	1
Sequence Labelling	1
BERT is effective	1
85.0	1
selectively moving vectors	1
It can be seen that our method, LSTM-LSTM-Bias, outperforms all other methods in F1 score and achieves a 3% improvement in F1 over the best method CoType (Ren et al., 2017).	1
during training	1
conditional probability of the target sequence given a source sequence	1
Transformer can generalize to other tasks	1
3 x10-4	1
unique words	1
l2 weight decay	1
unsupervised pretraining of language models on large corpora	1
a bag-of-means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams (up to 5-grams) from the training set and use their corresponding counts as features	1
max function	1
maximum loss-free compression rate	1
We conduct ablation tests on the ACE04 dataset reported in Table 2 to analyze the effectiveness of the various parts of our joint model. The performance of the RE task decreases (~1% in terms of F1 score) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task.	1
linear in length	1
Table 1 gives the overall results on ACE 2005. The first observation is that our model architecture achieves much better results compared to the previous state-of-the-art methods.	1
67%	1
parallelized	1
joint model that performs the two tasks of entity recognition and relation extraction simultaneously, and that can handle multiple relations together.	1
local and non-local contexts	1
text classification	2
series of modules	1
bidirectional sequential (left-to-right and right-to-left) LSTM-RNNs	1
In this paper, we investigate an end-to-end model to produce the tags sequence as Figure 3 shows.	1
SMT decoder	1
WMT 2014 English-to-German translation task	1
scaling factor	1
"Other" tag	1
Cloze-style queries	1
two capsule architectures	1
fine-tuning	2
68%	1
(partially) processed nested mentions	1
weighs the confidence of using sentence a in solving the task	1
much less parameters	1
CNN+MCFA	2
batch size	3
stochastic gradient descent (SGD)	1
an entity is considered correct if the boundaries and the type of the entity are both correct; a relation is correct when the type of the relation and the argument entities are both correct	1
Character Prediction	1
in same vector space	1
contextualized word representations	1
200	2
a certain variability of results	1
Approach	12
model the compositionality in text sequences	1
all competing models	1
LM embedding	1
fastest algorithm	1
relation extraction task	1
LSTM decoder	1
type and boundaries of the entities	1
all possible spans	1
set of context-sensitive filters	1
Unsupervised neural machine translation	1
state-of-the-art results on both subtasks	1
65536 experts (68 billion parameters)	1
significantly worse	1
Our Entity-Aware BERTSP gives comparable results to the top-ranked system (Rotsztejn et al., 2018)	1
inputs to both the query and the document attention mechanisms	1
Python with the TensorFlow machine learning library (Abadi et al., 2016)	1
entity pretraining	3
latter 400 characters	1
Nested Mention Recognition	1
by almost 20 points	1
NVIDIA V100 (32GB) GPUs	3
CNN+LSTM model	1
fully connected feed-forward network	1
Sentence-State LSTM for Text Representation	1
recurrent neural network	1
BioBERT v1.1 (+ PubMed)	1
deep neural network models	1
token character encoder	1
word and of KB triples	1
ADAM optimizer	1
SST	1
equilibrium	1
5 x 10-4	1
char ([25; 50; 100]) embedding dimensions	1
target network is 3	1
hashing trick (Weinberger et al., 2009)	1
SUBJ	1
Neural Semantic Encoders (NSE)	1
3% improvement in F1	1
Hyperparameters	32
100,000 steps or 12 hours	1
General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a)	1
our M-ACNN	1
+0.93 F1	1
QASent dataset	1
Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN)	1
position-aware attention mechanism over LSTM outputs (PA-LSTM)	1
separately	1
sampled uniformly	1
influence of various filter types	1
Code	1
5 points of F1	1
up to +13.37 BLEU points	1
best performing Match-LSTM model	1
large text corpus	1
previous work	1
LSTM hidden states	1
question and document	1
by 10.3	1
pruning distance K is varied	1
accuracy slightly	1
to 85.8%	1
perturbed input sentences	1
We also find that the LSTM-LSTM model is better than LSTM-CRF model based on our tagging scheme.	1
disturbance of some noise capsules	1
jointly extract entities and their relations	1
4:8 BLEU points	1
document	1
SCIVOCAB	1
networks	1
using the top LSTM layer	1
alternating search	1
Begin, Inside, End, Single	1
Our method outperforms the baseline method	1
sentence matching	1
pipelined methods	1
[0.2, 0.5, 0.7]	1
neural network architecture	1
increase of +1.79	1
sequence model	2
Experimental Setup	2
exogenous word weights	1
key n-gram phrase/words from the input document	1
any retraining	1
Bi-LSTM and the 4-layer CNN	1
norm is greater than 5	1
semantic representations of words	1
BioBERT	7
two different views of text sentences	1
Bi-LSTM when paired with greedy decoding	1
Vocabulary	1
some of the tokens from the input	1
surprisingly lower and highly stable	1
Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset	1
entity-aware self-attention mechanism	1
fine-grained feature abstraction	1
Large-scale Simple Question Answering	1
simple use of a distributed word representation	1
MACHINE TRANSLATION (SINGLE LANGUAGE PAIR)	1
same convolutional architecture	2
two-layer BiLSTMs	1
from 0.60 to 0.68	1
read/write modules	2
embedding of a word is computed	1
one hidden layer	1
Adversarial training (AT)	2
joint entity recognition and relation extraction	1
Attention	1
Google Production dataset	1
the feedforward layers, the LSTM component and the dependency structure	1
top single-model result (Luan et al., 2018),	1
BERT obtained a higher micro averaged MRR score (7.0 higher) than the state-of-the-art models.	1
both unigram and bigram filters	1
mean reciprocal rank score	1
Supervised and Semi-Supervised Text Categorization	1
Out-Of-Vocabulary (OOV) words	1
pre-trained BioBERT	1
relational information with regard to multiple entities	1
feedforward deep neural network (DNN)	1
highest F1 scores	1
LSTM-CRF (Lample et al., 2016)	1
Architectures of Capsule Network	1
adversarial training method to learn frequency agnostic word embedding	1
0.5 (0.1 for QA 10k)	1
memory	1
Baseline System	1
open-domain question answering	1
300 units	1
(1,2,3)	1
composed feature vector	1
original query	1
Ablation Results	1
Joint extraction of entities and relations	1
Adel and Schutze (2017)	1
entity-aware attention mechanism	1
back-propagation	1
the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture	1
GRU recurrent weights	1
input sentence and a conditional random fields	1
Subjectivity of sentences from movie reviews and plot summaries	1
GloVe (Pennington et al., 2014)	2
67.2 to 70.4	1
simple-to-comprehensive fusion of matching patterns	1
other datasets	1
Positional Encoding	1
multidomain tasks	1
by 23.7%	1
Sentence similarity	1
smaller quantities of data	1
~2% in both tasks	1
B(oundaries)	1
mixed results	1
some parts of the query	1
alternative recurrent neural network structure	1
GCN	1
AG's news corpus (AGs)	1
256 dimensional	1
receptive fields	1
Deep contextualized word representations	1
consecutive words in the sentence, and between the sentence-level state and each word	1
time	1
second-order interactions over the latent features via a tensor	1
pruning (i.e., using full trees as input)	1
Supervised training	1
~1% in terms of F1 score	1
SWEM-hier for sentiment analysis	1
BLEU=20.6	1
beta1 = 0.7	1
0.8% on MRR	1
number of layers is only one	1
task-specific loss	1
Ablation Analysis	4
two classification models	1
any NLP task	1
encoding sub-graph of the transformer architecture (Vaswani et al., 2017)	1
Training	1
character-level convolutional network (CL-CNN) (Zhang et al., 2015)	1
Universal Language Model Fine-tuning (ULMFiT)	1
small/large word CNN (Zhang et al., 2015)	1
+1.5 F1	1
main benchmark WebQuestions	1
MSRP dataset	1
Entity-Aware BERTSP	1
Model architecture	2
150 settings	1
In all the datasets examined in this study, we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset.	1
dissimilar parts explicitly	1
relevance of entity tags	1
sequence of state-changing triggers	1
low-dimensional vector embeddings	1
Subset selection from massive data	1
sentences of length up to 30 words	1
our lexical feature vector is complementary to standard word embeddings	1
our methods demonstrate clear advantage as a single model	1
contribution of neural components including pre-trained embeddings, the character-level LSTM and dropout layers	1
minibatch stochastic gradient descent (SGD) algorithm	1
ByteNet	1
1-to-1 proportion to the clean parallel data	1
512 memory cells	1
37.7	1
one data set	1
optimize the top of the list	1
most multi questions have their evidence distributed across contiguous sentences	1
forest where each outermost mention forms a tree consisting of its inner mentions	1
CL-CNN	1
ElMo (Embeddings from Language Models) representations	1
53.81	1
Efficient Text Classification	1
each layer	1
coreference annotation in ONTONOTES	1
preprocessing the input text	1
three popular biomedical text mining tasks (NER, RE and QA)	1
DBpedia, Yelp-bi, and Yelp-full	1
sufficient information	1
taking the advantage of RNN's capability	1
encoder and decoder	1
50-dimensional Glove (Pennington et al., 2014) word vectors	1
Neural machine translation	3
word in a translation	1
Classifying semantic relations between entity pairs in sentences	1
are truly good	1
Bidirectional Encoder Representations from Transformers	1
each layer of hidden states	1
corresponding accuracies	1
English Constituency Parsing	1
words appear in their natural order	1
read, compose and write operations	1
Bi-Ans-Ptr with bi-directional pre-processing LSTM	1
Specifically, we use three evaluation types, namely: (i) Strict: an entity is considered correct if the boundaries and the type of the entity are both correct; a relation is correct when the type of the relation and the argument entities are both correct, (ii) Boundaries: an entity is considered correct if only the boundaries of the entity are correct (entity type is not considered); a relation is correct when the type of the relation and the argument entities are both correct and (iii) Relaxed: we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given; a relation is correct when the type of the relation and the argument entities are both correct.	1
both accuracy and MSE	1
ReLU activation	4
2+% absolute higher NER and RC scores	1
relation classification (RC) component	1
Self Attention	1
Ablation studies	1
by 3.2%	1
than the word-by-word baseline system	1
convolutional neural network models (CNN) (Kim, 2014)	1
K = 300	1
importance of word-distance measures	1
different of +1.17	1
standard hyperparameters	1
another BiLSTM	1
standard convolutional layer	1
parallel data and back-translated data in a 1-to-1 proportion	1
diverse set of six benchmark NLP tasks	1
3-word window	1
LSTM/Bi-LSTM (Cho et al., 2014)	1
to inject supervision on intermediate activations of the network	1
up to four K80 GPUs	1
MCTest-500	1
all words in the other sentence	1
flattened	1
BERT: Bidirectional Encoder Representations from Transformers	1
100 feature maps each	1
predict the next word conditioned on previous words	1
label embedding	1
Tagging Scheme	1
4 out of 6 benchmarks	1
two sentence usability metrics	1
annotated data is difficult and expensive to collect due to the expertise required for quality annotation	1
Speed with fixed N	1
SWEM-based models	1
baseline result	1
NN parameters	1
table of phrase pairs	1
vectors	1
hidden layer	1
alternative lexical representation	1
ID-CNN out-performs the Bi-LSTM	1
biomedical domain corpora (PubMed abstracts and PMC full-text articles)	2
random 10%	1
overall (~ 1%)	1
two LSTM	1
to cover	1
out-of-vocabulary problem of neural models	1
common noun category	1
16 datatsets of Liu et al. (2017)	1
50-dimensional word embedding	1
Recurrent Neural Network Grammar	1
CBT	1
BioBERT results	1
document and the query	1
BERT achieved better performance than the state-of-the-art model	1
Text Preprocessing in Neural Network Architectures	1
fixed length	1
large-scale dataset of questions and answers based on a KB	1
Stack-LSTM	1
words are reordered based on a linearization of the sentence's dependency graph	1
low task-dependent loss	1
biggest improvement	1
85.4% accuracy score	1
large margin (>4% for both tasks)	1
state-of-the-art test performance	1
hidden layers	1
text-pair representations	1
a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET)	1
to the RNN hidden-to-hidden matrix	1
25 epochs	1
l=64	1
20 Newsgroups (20NG)	1
3 and 2	1
how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora	2
the baseline outperforms Katiyar and Cardie (2017)	1
a feature generator and a linear model	1
local order of words	1
1.0 F1	1
machine comprehension problem	1
Nesterov's accelerated growth	1
+3.55 F1	1
https://github.com/leuchine/S-LSTM	1
convolutional models (ARC-I & II, SENNA+MLP) perform favorably over bag-of-words models	1
biased loss	1
all the sentence vectors (e.g. Arabic, English, Korean, etc.)	1
proposed gates at the encoder and at the decoder	1
finetuning the LM and the classifier	1
LSTM and CNN compositional architectures	1
joint approach does slightly better than the pipeline approach	1
SCIBERT	1
single system	1
coefficients beta1 = 0.9 and beta2 = 0.999	1
grid searches	1
linear classifier	1
Method	2
simple feed-forward neural network	1
CHEMPROT dataset	2
recurrent computations for shared substructures	1
https://github.com/bekou/multihead_joint_entity_relation_extraction	2
expand this size	1
TACRED dev set	1
inferential links	1
use the Unfolding Recursive Autoencoder [19] to get a 100-dimensional vector representation of each sentence, and put an MLP on the top as in WORDEMBED	1
best translation performance	1
weights from BERT	2
sqrt(1/2)	1
All CNN models	1
models that use both feature sets significantly outperform other configurations	1
allowing interactions between the endpoints using the spanlevel FFNN	1
stronger modeling flexibility and capacity	1
averaged over five runs	1
context aware word representations	1
relation between representing sentences and matching them	2
Notably, by properly incorporating off-path information, our model outperforms the previous shortest dependency path-based model (SDP-LSTM).	1
address the lack of high-quality, large-scale labeled scientific data	1
tags sequence	1
supervised sequence tagging model	1
relation and entity recognition	1
both the memory capacity and the ease of training	1
(Gormley et al., 2015; Nguyen and Grishman, 2015; Fu et al., 2017; Shi et al., 2018)	1
network embedding method	1
extract entities together with relations using a single model	1
incorporating off-path information is crucial to relation extraction	1
Adam optimization algorithm	1
Text preprocessing	1
model learned from adversarial training also outperforms original one	1
conclude that these components contribute significantly to the effectiveness of our model	1
word sequence	1
LSTM-CRF model outperforms all other systems	1
0.01	1
Viterbi inference	1
dos Santos et al. (2016)	1
State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge	1
CSLM and the phrase scores from the RNN Encoder–Decoder	1
CoNLL 2000 chunking	1
directly classifying	1
over-fitting	1
1.0% absolute accuracy	1
pre-training steps	2
BIO (Beginning, Inside, Outside) encoding scheme	1
300-dimensional Multilayer Perceptron (MLP) layer	1
18 Mb	1
all previous work	1
Semantic Scholar (Ammar et al., 2018)	1
only the unigram filters	1
single questions (> 2%)	1
machine comprehension	1
entity recognition and relation extraction simultaneously	1
acceleration	1
Neural sequence model	1
feed-forward computation	1
Speed vs. increasing N	1
results, both in precision and recall	1
h = 5	1
categorizing documents	1
representation to fuse the left and the right context	1
entities and relations	1
one-layer architecture	1
window size	1
portion of nested mentions	1
cross entropy loss	1
BiLSTM-CRF-based multihead selection model	1
output embedding	1
individual (meaning-bearing) tokens	1
knowledge and representation sharing	1
large margin (2.8+%)	1
stack-LSTM model	1
S(trict)	1
300-dimension vector	1
Translation results	1
LSTM layer	1
representation loss	1
1150	1
one NVIDIA GeForce GTX TITAN X GPU	1
15 epochs	1
Bi-LSTM-CNN-CRF models of (Chiu and Nichols, 2016) and (Strubell et al., 2017)	1
40 epochs	2
our model architecture achieves much better results compared to the previous state-of-the-art methods	1
encoder (between layers 2 and 3)	1
best MAP	1
biased objective function	1
AWD-LSTM-MoS model	1
training set	1
Adding the reset gate	1
multi-head attention	1
vector gates	1
training	2
two decoder states	1
GRU	1
competing spans	1
logistic regression (LR) classifier	1
200K	1
states	1
stack of N = 6 identical layers	2
Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance	1
0.25	1
sum and transformation of word embedding vectors	1
both NER (~1%) and RE (~2%) tasks	1
consistent improvement in performance	1
https://github.com/dinghanshen/SWEM	1
hidden unit ([50; 128; 256; 300])	1
sigmoid scoring layer	1
exchanging information	1
multitask learning	1
prediction accuracies generally increase	1
Text Classification (CLS)	1
character-based word representation model	1
self-attention sub-layer	1
MCFA	2
performance is further improved	1
Extracting entities and their semantic relations from raw text	1
layer normalization	2
increasing model capacity without a proportional increase in computation	1
encoder-decoder model (NSE-NSE)	1
boundary model	2
single dropout mask	1
four datatsets	1
word-level cross entropy loss	1
[-0.001, 0.001]	1
pretrained language model based on BERT (Devlin et al., 2019)	1
end-to-end model	1
BC5CDR and ChemProt	1
followed by	1
not factored	1
End-to-end Model	1
resolution preserving	1
training on both datasets only	1
neural network for processing sequences	1
GNMT model	1
compositional coding	1
beta2 = 0.99	1
embeddings of rare words and popular words actually lie in different subregions of the space	1
As illustrated in Figure 1, our model architecture can be viewed as a mixture of a named entity recognition (NER) component and a relation classification (RC) component. Our NER component employs a BiLSTM-CRF architecture [10] to predict entities from input word tokens. Based on both the input words and the predicted NER labels, the RC component uses another BiLSTM to learn latent features relevant for relation classification. In contrast, our RC component takes into account second-order interactions over the latent features via a tensor. In particular, for relation classification we propose a novel use of the deep biaffine attention mechanism [7] which was first introduced in dependency parsing.	1
elementary operation	1
For the CoNLL04 dataset, there are two different evaluation settings, namely relaxed and strict. We observe that our model outperforms all previous models that do not rely on complex hand-crafted features by a large margin (>4% for both tasks).	1
popular and rare	1
similar and dissimilar components into a feature vector	1
dilated convolutions	1
Training large-scale question answering systems	1
Code Learning	1
encoder	3
encoder generates the latent representations	1
Lample et al. (2017)	1
BioBERT v1.1 (+ PubMed) outperformed the state-of-the-art models by 0.62	1
end-to-end neural model to extract entities and relations	1
capsule networks proposed in Sabour et al. (2017)	1
weight-sharing constraint	2
BioBERT v1.0 (+ PubMed) obtained a higher F1 score (2.80 higher) than the state-of-the-art models	1
Adam optimizer	3
several different perspectives	1
4 x 10-5	1
higher scores than BERT	1
CNN	2
model's effectiveness in handling nested mentions	1
Word Embedding layer	1
BLEU scores of 40.56 and 26.03	1
Pretrained BERT Variants	1
LSTM regularized by linguistic knowledge (LR-LSTM) (Qian et al., 2016)	1
greedy ID-CNN	1
eight	3
inserted	1
Bert-Base	1
conditional computation	2
MRE one-pass setting	1
Word2Vec	1
BooksCorpus	2
0.7%	1
PARALEX	2
to input embedding layers	1
maps each word in a sentence into vector representations	1
word2vec (Mikolov et al., 2013)	1
On the other hand, compared to the top single-model result (Luan et al., 2018), which makes use of additional word and entity embeddings pretrained on in-domain data, our methods demonstrate clear advantage as a single model.	1
0.85	1
300	4
entities	1
capsule networks	1
R(elaxed)	1
Phrase level opinion polarity from news data	1
each token's label independently	1
codes	1
best discrete codes that minimize the loss	1
a novel bidirectional filter generation mechanism	1
dimensional	1
learning rate	6
0.97	1
word2vec	1
performance of all three models peaks	1
OntoNotes 5.0 English NER	1
representations of questions and corresponding answers	1
best published results to date	1
+3.59 F1	1
simple, position-wise fully connected feed-forward network	1
1-4%	1
joint entity and relation extraction	1
higher scores than	1
global function	1
more elaborate features	1
We compare our method with several classical triplet extraction methods, which can be divided into the following categories: the pipelined methods, the jointly extracting methods and the end-to-end methods based our tagging scheme.	1
lexical semantics	1
sequence labeling	1
single machine with multi-cores	1
simple question answering	1
Question Answering	1
answers that consist of multiple tokens from the original text	1
Adverse Drug Events, ADE (Gurulingappa et al., 2012)	1
embedding-based QA system	1
encoder-shared model	1
Experiment 1: Preprocessing effect	1
log-linear model	1
Experimental setup	9
no pre-defined grammars or lexicons	1
each word embedding	1
algorithm inspired by transition-based parsing with states represented by stack LSTMs	1
combines dependency-based features with other lexical features.	1
1.01 higher test BLEU score	1
one or more stacked Long Short-Term Memory (LSTM) layers	1
shallow neural network	1
speedup solver	1
compressing word embeddings	1
methods that explicitly use the order (Wang and Manning, 2012)	1
each word w	1
same hashing function as in Mikolov et al. (2011)	1
to RNN layers	1
appreciable advantage	1
state-of-the-art models	1
repeat	1
Story-based QA	1
morphological features	1
Accuracy comparison	1
Short Sentence Processing	1
function of the entire input sentence	1
91.57%	1
38.77	1
number of experts	1
As we will show in Section 6.2, we find that our GCN models have complementary strengths when compared to the PA-LSTM.	1
language representation model	1
PMC	1
18% papers	1
Impact of classifier fine-tuning	1
training set size increases	1
filter types	1
novel alternating attention mechanism	1
The baseline model, described in detail in Bekoulis et al. (2018b), is illustrated in Fig. 1. It aims to detect (i) the type and the boundaries of the entities and (ii) the relations between them.	1
embeddings are used to extend, rather than replace, hand-crafted features in order to obtain state-of-the-art performance	1
each stack	1
multi questions (~ 0.3%)	1
Obtaining large-scale annotated data	1
cross entropy between ^v and the one-hot vector of the true answer	1
minibatch of 80 sentences	1
word vectors	2
CoType (Ren at al., 2017)	1
multilayer network	1
through a sequence of shift-reduce actions	1
ACE04	3
simple sum of embedding learned viaWord2Vec	1
7.4 BLEU points	1
ACE05	1
linear decomposition	1
training with global normalization	1
10	4
word level transfer	1
word-order information	1
most relevant information is concentrated	1
using labels that closely align with the task	1
16	2
1, 2, 4, 8 and 16	1
Situations With Adversarial Generations (SWAG) dataset	1
Experiments (supervised)	1
the vector representation back to a variable-length target sequence	1
sentence matching tasks	1
a random sample of 1.14M papers	1
refined word embeddings	1
any pretrained word or sentence embeddings	1
depLCNN+NS	1
deep bidirectional Transformer	1
all the biomedical QA datasets	2
20	2
25	2
5 epochs	2
pre-trained 300-D Glove 840B vectors and 100-D Glove 6B vectors	1
uniform distribution	1
jointly extracts entities and relations using structured perceptron on human-annotated dataset	1
3%	1
model outperforms all previous models that do not rely on complex hand-crafted features	1
fully connected capsule layer	2
topic prediction tasks	1
four layers	1
30	1
1.2% improvement in F1	1
previous best system	1
32	5
more sophisticated region embedding	1
bigram information	1
source and target sequences	1
BLEU	1
Wikipedia 2014 and Gigaword 5 (Pennington et al., 2014)	1
all of the four layers are shared	1
our ID-CNN	1
texts	1
5-gram filters into win-4	1
performing prediction	1
+2.11 F1	1
0.0001	3
We have developed our joint model by using Python with the TensorFlow machine learning library (Abadi et al., 2016).	1
Regularization	1
SemEval Dataset	1
48	1
highest performance	1
4.58	1
jointly extracting methods	1
two capsule frameworks	1
classification accuracy	2
context sentences	1
maximum sequence length	1
PubMed	1
to embedding layers	1
two independent encoders	1
5%	1
SENNA+MLP/SIM	1
Ablation study	1
systems that do not use external data	1
increase of about +0.74	1
300-dimensional (d=300)	1
50	5
sequentially encodes the representations of self attention layer	1
lowercasing and lemmatizing	1
arranged in sets of five	1
eight of the nine datasets	1
another NSE	2
Semantic matching	1
single DAN encoder	1
When predicting multiple relations in one-pass, we have 0.9% drop on Macro-F1, but a further 0.8% improvement on Micro-F1.	1
Architecture	4
vector-output capsules	1
character-level LSTM	1
search mechanism	1
one-hot CNN with one 200-dim CNN tv-embedding	1
dropout rate	1
pre-trained Senna word embeddings	1
the missing query word, the query, and the document	1
The input of our model is a sequence of tokens (i.e., words of the sentence) which are then represented as word vectors (i.e., word embeddings).	1
misspellings and emoticons	1
64	2
Bi-LSTM	2
allow	1
AWD-LSTM language model (Merity et al., 2017a)	1
WordEmbed	1
input tokens and output tokens to vectors of dimension dmodel	1
two AE	1
Sentence Classification	1
bAbI QA 10k	1
matrix of basis vectors	1
proposed RNNsearch outperforms the conventional RNNencdec	1
similar parts of the sentence pair	1
MPQA	1
Our best performing system outperforms the top leaderboard system	1
character level embeddings	1
70	1
Adadelta update rule	1
The MoE model	1
applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree	1
ASPEC dataset	2
role of preprocessing the input text	1
Naver Smart Machine Learning (NSML) (Sung et al., 2017	1
four components in different ways	1
1e-5	2
Unlike traditional incremental end-to-end relation extraction models, our model further incorporates two enhancements into training: entity pretraining, which pretrains the entity model, and scheduled sampling (Bengio et al., 2015), which replaces (unreliable) predicted labels with gold labels in a certain probability. These enhancements alleviate the problem of low-performance entity detection in early stages of training, as well as allow entity information to further help downstream relation classification.	1
better results than all previously reported models	1
39% lower	1
300-dimensional CBOW Word2vec embeddings (Mikolov et al., 2013a)	1
1e-3	1
full corpus	1
extracting multiple entity-relations from an input paragraph	1
Model Architecture	1
80	1
512 dimensional sentence embedding	1
whole training process converges	1
take the matching model in [13] and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer	1
Label Smoothing	1
multi-label head selection problem	1
framework of Memory Networks (MemNNs)	1
Entity-aware Attention layer	1
The GANs	1
ablation tests	2
rate of 0.5	2
reading process	1
VDCNN	1
Stanford Question Answering Dataset (SQuAD v1.1)	1
to (un-interpretable) vectorial representations	1
loss	1
{1, 0.1, ..., 1.0e-4}	1
uRAE+MLP	1
logistic regression	1
Pointer Net (Ptr-Net) model	1
almost 1.0 BLEU	1
span encoder and the passage-independent question representation	1
almost 5%	1
performance boost	1
Unconstrained English-Estonian and Estonian-English NMT systems	1
Fine grained question classification sourced from TREC	1
Low-shot learning	1
100 labeled examples	1
word embeddings	3
WiFiNE (Ghaddar and Langlais, 2018)	1
Simple word-by-word matching	1
substantial margin	2
the answer	1
increasing factors of dilation	1
150 or 300	1
model robust to input perturbations	1
dropout	8
Relaxed	1
We present a novel end-to-end model to extract relations between entities on both word sequence and dependency tree structures.	1
two bidirectional layers	1
capitalization ([10; 20; 30])	1
embedding layer	4
models that only use word level transfer	1
n-grams	1
Gumbel-softmax trick (Maddison et al., 2016; Jang et al., 2016)	1
We initialized word vectors via word2vec (Mikolov et al., 2013) trained onWikipedia8 and randomly initialized all other parameters.	1
Sentiment Analysis	1
41.9% and 42.2%	1
across	1
orthogonal	1
12GB memory	1
constructing word embeddings with drastically fewer parameters	1
two-channel CNN operation	1
use the SENNA-type sentence model for sentence representation	1
sentence similarity	1
novel techniques	1
biomedical domain	2
Experimental Results	1
S-LSTM gives significantly better results compared with BiLSTM	1
Dependency-based models	1
The baseline model outperforms the state-of-the-art models	1
https://tfhub.dev/google/universal-sentence-encoder/4	1
linear in the length	1
Test Classification	1
DAN	1
of most sequences	1
Results	56
six out of nine datasets	2
reading comprehension	1
wv-2LSTMp (word-vector bidirectional LSTM with pooling)	1
+7.31 in F1	1
distinct nature of various NLP tasks	1
current state-of-the-art word representation models such as BERT need to be trained on biomedical corpora	1
CNNrand/CNN-static/CNN-non-static (Kim, 2014)	1
100-billion-word News corpus (Mikolov et al., 2013).	1
Finally, we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax. Assuming independent distribution of labels (i.e., softmax) leads to a slight decrease in the F1 performance of the NER module and a ~2% decrease in the performance of the RE task.	1
single intermediate layer	1
dimensions	1
same block of dilated convolutions	1
normal CNN	1
0.0003	2
Number of weight-sharing layers	1
residual connection	1
Machine Reading	1
decoder (between layers 1 and 2)	1
nested hierarchical structure of entity mentions should be taken into account	1
RG65	1
C-GCN model further outperforms the strong PA-LSTM model	1
LSTM layers in the encoder	1
Moses with default settings	1
60 to 200 epochs	1
+2.43 F1	1
the MoE model	1
from	3
coupled language model (LM) objective	1
the best previously reported models (including ensembles)	1
input sequence	2
Sskip model outperforms our feature vector approach	1
d = 300	1
sentence level transfer learning	1
Frozen BERT Embeddings	1
end-to-end neural architecture	1
300 dimensional GloVe embeddings	1
WEAT	1
constant learning rate	1
corresponding word is independent of the extracted results	1
decoder	4
document classification experiments	1
Systems	1
shared encoding memory	1
a set of positions	1
sequence	1
multi-task learning	1
BiLSTM-CRF architecture	1
Biomedical Domain	1
an F1 score of 71.2%	1
detected entities	1
1k datasets	1
an entity is considered correct if only the boundaries of the entity are correct (entity type is not considered); a relation is correct when the type of the relation and the argument entities are both correct	1
a single encoding model	1
BLEU score of 28.4	1
learned filters	1
English-to-French	1
input to each LSTM layer	1
convolutional capsule layer	1
300-dimensional word2vec (Mikolov et al., 2013) vectors	1
scalar-output feature detectors of CNNs	1
lenient accuracy	1
state-of-the-art accuracy	1
LSTM-LSTM-Bias, outperforms all other methods	1
a recursive model that generalizes the LSTM to arbitrary tree structures.	1
error	1
we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account	1
8.3%	1
question answering when reasoning over multiple facts is required	1
Finetuning BERT	1
hyperparameters	2
50% over this baseline	1
large margin (2.5+%)	1
novel tagging scheme	1
more sophisticated deep learning models	1
of dimension 100	1
English-to-German translation	1
objective loss	1
named entity recognition and relation classification	1
-0.3 BLEU points decline	1
larger datasets tend to perform better	1
one-pass encoding MRE solution	1
corresponding matches	1
performance	3
hidden states of all words simultaneously	1
search gates	1
two recurrent neural networks (RNN)	1
4 percent absolute on validation and 3.4 on test	1
an alternative neural network structure	1
standard CNN model	1
training from scratch	3
compositional model	1
accuracy	1
normal distribution	1
dependency path-based counterpart (K = 0)	1
BioBERT outperformed the state-of-the-art models	1
models that do not make use of transfer learning	1
5 groups of filters	1
ELMo (Embeddings from Language Models) representations	1
combination of six factored data NMT systems	1
biases	1
new state-of-the-art performance	1
convolutions in the decoder CNN	1
ACL-ARC (Cohan et al., 2019)	1
single MoE-augmented model	1
context-sensitive representations	1
AE	1
is done	1
AG	1
each input word	1
BERTSP with entity indicators on input layer	1
NewsTest 2015	1
Training is performed using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 10-3.	1
NewsTest 2014	1
large margin	1
dilation rates	1
multiple relations with one-pass encoding	1
LSTM-LSTM (Vaswani et al., 2016)	1
150 hidden units	1
1M steps	2
source sentence	1
single LSTM layer	1
WSJ dataset	1
300 dimensional	1
Shortest Dependency Path LSTM (SDP-LSTM) (Xu et al., 2015b)	1
when, and why, simple pooling strategies	1
shared-latent space	1
"next sentence prediction" task	1
pretrained language model based on BERT but trained on a large corpus of scientific text	1
machine comprehension on the challenging MCTest benchmark	1
Prediction accuracies vs. increasing K	1
Impact of pretraining	1
TensorFlow	1
a fixed number of iterations	1
5e-5	2
all the filters in win-2 plus trigram filters	1
best hyperparameters	2
pre-training	3
decreased	1
Tree-LSTM	1
RNN Encoder-Decoder	3
82%	1
MR data set	1
CR	1
target word	1
embeddings trained on a simple tokenized corpus	1
fixed depth	1
DS-Joint (Li and Ji, 2014)	1
maximize the conditional probability of a target sequence given a source sequence	1
sentence to sentence	1
non-dilated CNN architecture with the same number of convolutional layers	1
initialize	1
AG's news	1
Our NSE models outperformed the previous state-of-the-art models	1
irrelevant information from the tree while maximally keeping relevant content	1
all three models are less effective	1
epsilon = 10-6 and rho = 0.95 (Zeiler, 2012)	1
capacity of a neural network to absorb information is limited by its number of parameters	1
Batch sizes	1
Exploiting scale in both training data and model size	1
achieves	1
English Wikipedia	2
dependency view	1
ten benchmark datasets	1
captures the meaning of the correlation between words based on multi-head attention	1
alternative LSTM structure for encoding text	1
all the words	1
entire input sequence	1
helped	1
the class	1
third sub-layer	1
ensemble of 5 models	1
ones using external labeled data like gazetteers	1
at least +5.01 BLEU points	1
Extracting semantic relations between entities in text	1
directional self-attention	1
mini-batch	1
words in a given sentence are in right sequential order	1
slight decrease in the F1 performance of the NER module	1
other models	2
dimensionality of tv-embeddings	1
evaluates whether the most similar words of a given word in the embedding space are consistent with the ground-truth	1
each recurrent step	1
exotic character combination	1
ordering and identity of all the other words	1
surprisingly good performance	1
recurrent computation	1
biomedical text mining	1
End - to - End Relation Extraction	1
BioBERT v1.0 (+ PubMed + PMC)	1
Transformer	2
predict the original vocabulary id of the masked word based only on its context	1
LR-LSTM	1
a matrix	1
across all LSTM time-steps	1
two ways	1
pre-training BERT	1
Our greedy model is out-performed by the Bi-LSTM-CRF	1
+1.13 F1	1
AdaGrad (Duchi et al., 2011)	1
n-gram features at different positions of a sentence	1
DYNET v2.0	1
all of the experiments	1
Single models	1
models trained on a single QA dataset perform poorly on the other datasets	1
multi-head self-attention mechanism	1
tree-structured LSTM (Tree-LSTM) (Tai et al., 2015)	1
data size is generally much larger than feature length	1
Word Representation	1
IMDb	2
encoding memory	1
DMN	1
joint modeling of entities and relations	1
ACE datasets	1
baseline embedding matrix	1
through a minibatch of 30 instances	1
BERT for the biomedical domain	1
almost 5% accuracy improvement	1
almost no rise of selection time	1
single model	1
surprisingly well	1
large, unlabeled corpus	1
NSE or LSTM	1
extension to the encoder-decoder model	1
coverage of existing systems	1
neural networks	1
meta network	1
Comprehension of unstructured text by machines, at a near-human level	1
iterative inference process	2
Named Entity Recognition (NER)	1
Theano (Bastien et al., 2012)	1
Semantic textual similarity (STS) between sentence pairs scored by Pearson correlation with human judgments	1
effect of each corpus on pre-training	2
output of the final LSTM layer	1
Text Processing	1
87.6	1
final MLP layer	1
loss on development data does not decrease	1
In addition, we also compare our method with two classical end-to-end tagging models: LSTM-CRF (Lample et al., 2016) and LSTM-LSTM (Vaswani et al., 2016). LSTM-CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence. Different from LSTM-CRF, LSTM-LSTM uses a LSTM layer to decode the tag sequence instead of CRF. They are used for the first time to jointly extract entities and relations based on our tagging scheme.	1
similar	1
the state-of-the-art on both datasets	1
transfer learning from the DAN encoder	1
1k data	1
40%	1
ability to rerank answers	1
0.013	1
Building computers able to answer questions on any subject	1
word embedding layer	2
sparse combination of the experts to process each input	1
system	2
authors' baseline ESIM+ELMo system	1
BERTBASE and BERTLARGE outperform all systems on all tasks	1
400	1
number of classes is large	1
50% dropout	1
Our MMA-NSE attention model exceeds the NASM	1
mapping a query and a set of key-value pairs to an output	1
CRF loss layer	1
relation predictions	1
list of capsules	1
trainable gating network	1
predefined classes	1
by another 9.7 F1	1
simple linear model with rank constraint	1
answer questions about the contents of documents	1
Baselines	8
Machine comprehension of text	1
chunks of input sentences	1
two methods	1
Capsule-A	1
Capsule-B	2
preprocessing techniques	1
phrase-based SMT framework	1
end-to-end (joint) modeling of entity and relation	1
decoder stack	1
word embeddings from a word2vec skip-gram model	1
C-GCN model outperforms all existing dependency-based neural models	1
a corpus of news data (Mikolov et al., 2013)	1
CNN or the LSTM encoder	1
1.3 F1	1
two million parameters	1
outperforms Enc-Dec model	1
SP-Tree (p<0.01)	1
ensembling	1
systems with hand-crafted features, including Ratinov and Roth (2009) that use gazetteers	1
fine-tune BioBERT	3
In addition, we find our model improves upon other dependency-based models in both precision and recall.	1
8 NVIDIA P100 GPUs	1
quadratic number of possible answers	1
a variable-length source sequence to a fixed-length vector	1
LSTM-CRF	1
specific input sentences	1
0.5-0.7	1
cnn library	1
oh-2LSTMp outperforms SVM and the CNN	1
semantic matching functions	1
CoNLL 2003 NER	1
model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output	1
MC	1
WMT14 English-German	1
CoNLL	2
BioBERT achieves higher scores than BERT	1
only use the LSTM hidden states as inputs	1
MR	1
sentiment analysis	1
Experiment	1
BERTBase model	2
character-to-character machine translation	1
fast inference	1
Residual connection	1
Huffman coding tree (Mikolov et al., 2013)	1
S-LSTM outperforms BiLSTM significantly	1
a lowercased PTB tokenized string	1
96.37 mean F1	1
similar component	1
existing recurrent and convolutional networks	1
large margin (2.0+%)	1
TREC-6	2
0.009	1
2 epochs	1
depth of 16	1
Penn Treebank (PTB)	1
latent representations	2
0.001	4
ensemble	1
BioBERT achieved the highest F1 scores	1
50 epochs	2
0.003	1
remove the label embeddings layer	1
bidirectional tree-structured (bottom-up and top-down) LSTM-RNNs	1
interleaved bi-directional architecture	1
learn context-sensitive convolutional filters for text processing	1
all competing state-of-the-art approaches	1
complex architectures	1
BLEU score	2
"masked language model" (MLM) pre-training objective	1
single incrementally-decoded NN structure	1
Transformer models achieved better results than the MLSTM-based models	1
discriminator	4
Models that make use of sentence level transfer learning	1
0.1	4
For the NER task, we adopt the BIO (Beginning, Inside, Outside) encoding scheme.	1
0.2	2
200 to 600	1
0.3	1
0.5	5
Primary Capsule Layer	1
0.6	1
0.8	1
0.9	1
+-2.4%	1
robust inductive transfer learning	1
output of the encoder stack	1
previously mentioned unidirectionality constraint	1
LINE (Tang et al., 2015)	1
FCM (Gormley et al., 2015)	1
each capsule	1
decoding layer	1
QA	1
d is 512	1
by 3.2 F1	1
fastText	2
performance of standard neural text classification models	1
IMDb and AG	1
Our full model, with the structured fine-tuning of attention layers, brings further improvement	1
conditional probability of each target word	1
language modeling and machine translation	1
embedding space	2
models	2
source network is 3	1
RE task	1
very efficient	1
most recent published result (AS Reader)	1
bAbI story-based QA 10k dataset	1
tokens in the sequence	1
Computer Science Domain	1
10-fold cross validation	1
SWEM	5
six sets of five blocks	1
only one layer is shared in our system	1
sliding window	1
degraded	1
more n-grams	1
LSTM-CRF model	2
development set	1
three out of the four datatset	1
RE	2
MultiR (Hoffmann et al., 2011)	1
To analyze the contributions and effects of the various components of our end-to-end relation extraction model, we perform ablation tests on the ACE05 development set (Table 2).	1
domain independent framework by jointly embedding entity mentions, relation mentions, text features and type labels into meaningful representations	1
Viterbi-decoding Bi-LSTM-CRF and ID-CNN-CRF and greedy ID-CNN	1
compressed model	1
Attention-CNN	1
degrades	1
context aware representations of words in a sentence	1
beta1 = 0.9, beta2 = 0.98 and epsilon = 10-9	1
Deep-ED	1
RW	1
Machine Translation	3
each time	1
single model Deep-Att performance	1
LSTM-LSTM	1
general framework of 'region embedding + pooling'	1
faster training	1
ARSS	1
K codeword vectors	1
5e-5, 4e-5, 3e-5, and 2e-5	1
TACRED Dataset	1
WMT'14 En->Fr and En->De benchmarks	1
Stanford neural dependency parser	1
simple QA	1
initial learning rate	3
N = 1	1
CoNLL 2003 NER task	1
pre-trained word embeddings	1
constrained scenarios	1
FRequency-AGnostic word Embedding (FRAGE)	1
appropriate parent in the subsequent layer	1
computationally matched baseline	1
end-to-end methods	1
three datasets	1
modest performance loss	1
distributional representations	1
19% lower perplexity on the dev set	1
dimension 32	1
Impact of LM fine-tuning	1
relations	1
{0.05, 0.1, 0.25, 0.5}	1
Conditional computation, where parts of the network are active on a per-example basis	1
Tensorflow (Abadi et al., 2016) library	1
all weights	1
Google’s publicly available embeddings	1
baseline model	2
RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a)	1
Reranking	1
MLP	1
features	1
two bidirectional GRUs	1
Machine Comprehension on Sparse Data	1
two ways to learn refined word embeddings	1
sequence tagging tasks	1
Universal Sentence Encoder	1
neural machine translation	1
lack of task-specific tuning	1
sub-states for individual words and an overall sentence-level state	1
attentive pooling network	1
entity tag sequence	1
LSTM-CRF model significantly outperforms all previous methods	1
5-layer CNN	1
ADAMAX (Kingma & Ba, 2015)	1
4.5% and 7.0% respective average accuracy improvement	1
each of the sub-layers	1
5.30	1
We use dropout (Srivastava et al., 2014) to regularize our network.	1
JNLPBA	1
various convolutional filters	1
Adam optimizer (Kingma and Ba, 2014)	1
Removing character embeddings also degrades the performance of both NER (~1%) and RE (~2%) tasks by a relatively large margin.	1
SNLI dataset	1
50d LSTM states	1
single lookup in the memory	1
10% of the training	1
some partial information about the local word order	1
next search step	1
parameters	2
align and translate jointly	1
both the CNN baseline and the ACNN model	1
to layers	1
semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems	1
slightly lower compression rate	1
1; 2; 4; 8 and 16	1
back-translation	1
additional labeled data	1
10, 16, 32, or 64	1
encoding layer	1
System	2
Quantitative Analysis	1
10k dataset	1
BERTSP with position embedding on the final attention layer	1
word embeddings alone	1
hierarchical composition for sentences	1
novel path-centric pruning technique	1
transformation matrices	1
variable sized encoding memory	1
ensemble classifier	1
WS	1
maximally 500 epochs	1
200K iterations	1
bidirectional model	1
lp(0<p<=1)-norm based measure	1
gradients	2
sequential view	1
two types of models	1
5.5%	1
decay rate	1
Position-wise Feed-Forward Networks	1
pre-trained language models	1
state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification	1
Dynamic Routing	1
rank-100 matrices	1
model outperforms the model of Katiyar & Cardie (2017)	1
iterative manner	1
Decoder	1
use AT for the tasks of entity recognition and relation extraction	1
error reduction	1
0.65 F1 points on average	1
mini-batch size	3
all versions of BioBERT	1
pre-training strategies	2
gradient clipping of 3.0	1
Classification	1
one-dimensional convolutional neural networks (CNN)	1
When compared with the traditional methods, the precisions of the end-to-end models are significantly improved.	1
entity representations and feedforward layers	1
91.93 mean F1	1
hidden dimension size	1
43.9% and 22%	1
some information about the relative or absolute position	1
optimize all models	1
single layer for the forward and backward LSTMs	1
BIES	1
TREC data set	1
sentiment analysis datasets	1
additional features	1
word embeddings learned in several tasks are biased towards word frequency	1
system's state	1
one-hot CNN	1
S-LSTM	1
Augmented Lagrange Multiplier (ALM)	1
Full Ranking	1
Movie review snippet sentiment on a five star scale	1
decomposition operation	1
MRE	1
hierarchical softmax (Goodman, 2001)	1
2e-5	1
poor translation performance	1
sentences	1
scheduled sampling	2
64-core Intel Xeon E7-4820 @ 2.00 GHz	1
Machine Comprehension	1
MRR	1
English-to-German	1
general purpose	1
3.6 and 5.6 points	1
Pretraining	1
our tagging scheme	2
Dependency Parsing (DEP)	1
multiword-wise embeddings	1
CNN/LSTM compositional functions	1
Deep-Att	1
novel use of the deep biaffine attention mechanism	1
Machine comprehension	1
We implemented our model using the cnn library. We parsed the texts using the Stanford neural dependency parser (Chen and Manning, 2014) with the original Stanford Dependencies	1
used	1
alleviate	1
ontology classification problem	1
Relation extraction	2
GeForce GTX 1080 GPU	1
transfer learning from the transformer based sentence encoder	1
NER part of SciERC (Luan et al., 2018)	1
4-layer CNN	1
different semantic features	1
primary capsule layer	1
NER task	3
linear distance between matched words	1
Embeddings and Softmax	1
all datasets	4
n-gram convolutional layer	1
different preprocessing techniques	1
hidden state size	1
SciBERT outperforms BERT-Base	2
The input is a sequence of tokens (i.e., sentence) w = w1, ....,wn.	1
IWSLT14 German-English datasets	1
~2% decrease in the performance of the RE task	1
+1.3 F1	1
62 - 63%	1
decoder output to predicted next-token probabilities	1
due to small size of the 500-examples test set	1
system of (Luo et al., 2015) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks	1
for each entity pair, it takes the hidden states, adds the relative position embeddings corresponding to the target entities, and finally makes the relation prediction for this pair	1
TREC	1
standard bi-directional encoder	1
The whole network	1
previously published results	1
any KB	1
boundaries evaluation	1
Finally, we compare our model with previous work (Li et al., 2016, 2017) on the ADE dataset. However, our model is able to outperform both models using the strict evaluation metric. We report an improvement of ~2% in the NER and ~3% in the RE tasks, respectively.	1
500 characters each	1
bidirectional pre-training for language representations	1
non-linear map	1
forward and backward recurrent neural networks (RNN)	1
Residual Dropout	1
mini-batch (100 ~ 200 in sizes)	1
text representation	1
context-sensitive convolutional filters	1
WikiQA dataset	1
general domain corpora and biomedical corpora	2
Boundaries	1
smaller TREC-6	1
softmax layer	2
our LSTM with two 100-dim LSTM tv-embeddings	1
loss function	1
1024 units	1
Including vector gates	1
more informed query	1
AS Reader ensemble	1
4.8%, 18.2%, 2.0%	1
ngrams (Zhang et al., 2015)	1
test error	1
for the token character encoder	1
Hardware and Schedule	1
label embeddings	1
CoNLL04 dataset	1
BLSTM layer	1
three of the four data sets	1
individual words from hypothesis and text, at various scales	1
a new state-of-the-art	1
mini-batch stochastic gradient descent (SGD)	1
attention	1
child-parent relationships	1
a bidirectional LSTM	1
two linear transformations	1
10-fold cross-validation similar to Li et al. (2017)	1
For the pipelined methods, we follow (Ren et al., 2017)’s settings: The NER results are obtained by CoType (Ren et al., 2017) then several classical relation classification methods are applied to detect the relations. These methods are: (1) DS-logistic (Mintz et al., 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al., 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al., 2015) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction.	1
3B-word UMBC WebBase corpus (Han et al., 2013)	1
win-l	1
LET method	1
We find that under the conventional with-entity evaluation, our C-GCN model outperforms all existing dependency-based neural models on this separate dataset.	1
entity-centric representations	1
gradient norms	1
dimensions of intermediate layers	1
average accuracy of QRN's '6r200' (6 layers + reset gate + d = 200) model outperforms all previous models	1
micro averaged F1 score	1
dropouts	1
output of each capsule	1
two sets of back-translated data	1
probability distribution	1
summary of its inference process	1
downstream task impact of increasing the pre-trained bi-LM size from two to four layers	1
two GCN models and the Tree-LSTM	1
sophisticated hidden unit	1
subset selection	1
multi-head joint model	1
codebooks	1
92%	1
set of relevant encoding memories simultaneously	1
same architecture as BERT	1
external features	1
model the compositionality in variable-length text sequences	1
as	1
tag sequence	1
small and medium-sized datasets	1
win-5	1
BLEU=35.9	1
IWSLT14 dataset	2
win-3	1
win-4	1
each language	1
win-2	1
schema	1
entity recognition label	1
higher micro averaged MRR score	1
grid search	1
predictions of tokens from the input sequence	1
transfer learning using sentence embeddings	1
Miwa and Sasaki (2014)	1
Optimizer	1
popular words from rare words	1
5e-5, 3e-5 or 1e-5	1
Adam optimizer (Kingma & Ba, 2015)	1
{unigram, bigram, trigram}	1
single CNN model (best overall performance in seven of the nine datasets)	1
by	1
translation performance of our system	1
both datasets	2
fine-tuned model	1
set	1
character embeddings	2
character-based LSTMs	1
2 out of 3 biomedical datasets	2
words	1
Model settings	1
94%	1
residual connections	1
85.6%	1
the need for words	1
temporal (one-dimensional) ConvNets	1
30 residual blocks	1
linear combination of the vectors	1
SWEM-max	1
multi-layer recurrent networks	1
sequence layer	2
512-D LSTM inputs	1
token-wise representation	1
continuous space incrementally	1
Experimental results	2
Document Classification	1
RE results	3
ADE	1
highest average scores	1
NER and RC components as independent networks	1
Experiment 2: Cross-preprocessing	1
deep neural network (Pascanu et al., 2014)	1
encoding of the context at each position in the sequence	1
multiple downstream tasks	2
joint extraction of named entities and relations between them, without any hand-crafted features	1
10 asynchronous training threads	1
gradual unfreezing	1
https://github.com/berlino/nest-trans-em18	1
learning rate decay	1
We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 (both for the entity and the relation scoring layers).	2
our GCN models have complementary strengths	1
K = 100 feature maps	1
simplifying the recurrent update	1
approximately 1% on MAP	1
Scaled Dot-Product Attention	1
bAbI story-based QA 1k, bAbI dialog and DSTC2 dialog	1
simple deep neural networks	1
Adadelta	1
both tasks	3
capsule network	1
Experimental Settings	3
multiple relations extraction	1
Language model pre-training	1
Ablation analysis	2
5.0	3
sequential sliding window	1
ACE04 dataset	1
precisions of the end-to-end models	1
SWEM model	1
unconstrained models	1
BERT with structured prediction only	1
90% of the connections	1
text categorization	1
3 epochs	3
two one-layer LSTM	2
both entity and relation labels	1
1e-3 learning rate	1
S-LSTM gives the best results on 12	1
cross-language translation	1
represent each short-text as the sum of the embedding of the words it contains. The matching score of two short-texts are calculated with an MLP with the embedding of the two documents as input	1
a new type of linear connections	1
overfitting	2
Encoder and Decoder Stacks	1
the sentential component	1
Keras (Chollet, 2015) library	1
approximately three days	1
allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly	1
softmax function f	1
point-by-point manner	1
Adam (Kingma and Ba, 2014)	2
filter generation module	1
500	1
standard deviation fixed to 0.01	1
10 epochs	1
BERT-Base	1
improving generalization from the training to the test set	1
99%	1
Answer Sentence Selection	1
improves	2
Our full model, with the structured fine-tuning of attention layers, brings further improvement of about 5.5%, in the MRE one-pass setting, and achieves a new state-of-the-art performance when compared to the methods with domain adaptation.	1
results of a pipeline approach	1
8GB memory	1
improved	1
Stack-LSTM also consistently presents statethe-art (or close to) results	1
512	2
word-by-word perspective	1
novel end-to-end model	1
slanted triangular learning rates	1
context	1
at least 1.6 F1	1
model	7
Character-Level Machine Translation	1
transition-based system	1
tasks	1
OpenAI GPT	1
results of CNN or LSTM	1
in	1
both CNN and LSTM compositional functions	1
shared memory access	1
stochastic gradient descent	3
+0.49 F1	1
different languages	1
first 100 characters as the minimum context	1
computation from the hidden state in the decoder to the output	1
Ablation results	1
Application	1
every dilation width d inputs	1
predict entities from input word tokens	1
word-level sliding window	1
two LSTM layers	1
nested mentions	1
answering Cloze-style queries	1
random seeds	1
char-CNN and char-CRNN	1
LSTM-based decoding layer	1
relations between entities	1
Understanding unstructured text	1
[2, 8, 32, 128, 512]	1
passage-aligned question representation	1
4000	1
11 of the 12 language pairs (by as much as 5.84 points)	1
multi-instance learning algorithms to combat the noisy training data	1
detect entity mentions and recognize their semantic relations simultaneously from unstructured text	1
word compositionality	1
information exchange	1
CoNLL-2003	1
QRN's '2r' (2 layers + reset gate + d = 50) outperforms all other models	1
NER	2
string matching	1
traditional word type embeddings	1
multiple entity-relations extraction task	1
Deep Averaging Network (DAN)	1
10M bins	1
dropout ([0:25; 0:5; 0:65])	1
self usability	1
Long Short-Term Memory (LSTM)	1
learning perspective	1
second	1
7+% higher NER and RC scores	1
sequential data	1
about 0.4 seconds	1
predecessor and successor simultaneously	1
embedding vectors	2
document modeling	1
model the nested structure of mentions	1
as a kind of raw signal at character level	1
learn latent features relevant for relation classification	1
smaller data set	1
similarity matches	1
2 inputs	1
SQuAD dataset	1
input and forget gates	1
five blocks in each set	1
did not bring further improvements	1
states of input, stack and action history	1
a computational perspective	1
Word Similarity	2
each sentence with nested mentions	1
network from seeing future tokens in the target sequence	1
1.0	1
fixed set of filters	1
0.986 TB	1
discriminative fine-tuning	1
our one-hot bidirectional LSTM with pooling (oh-2LSTMp) outperforms word-vector LSTM (wv-LSTM)	1
scores	1
4 ~ 8	1
Improved Neural Network Named-Entity Recognition	1
weighs the confidence of using sentence a in fixing sentence b	1
Optimization	1
top N function	1
NVIDIA Titan Xp (12GB) GPU	3
long-term dependency problem of most RNN-based models	1
sampling from an isotropic zero-mean (white) Gaussian distribution	1
overall performance	1
of	1
The results on SemEval 2018 Task 7 are shown in Table 3. Our Entity-Aware BERTSP gives comparable results to the top-ranked system (Rotsztejn et al., 2018) in the shared task,	1
new state-of-the-art	1
BERT	2
N-gram Convolutional Layer	1
CoNLL04	3
context vectors	1
on	6
small TREC-6	1
equivalent derivation	1
by more than 5%	1
input embeddings for words and bi-grams	1
initialized randomly	1
machine reading comprehension tasks	1
sequence of tokens	1
Semi-supervised experiments	1
three filters	1
size 3	1
0.001 (0:0005 for QA 10k)	1
85.2%	1
back-propagation algorithm	1
SOTA	1
100	3
overall F1 of 0.4% for the EC and 0.8% for the NER tasks	1
new SOTA results	2
companion QA set	1
comparable MRR	1
20% input dropouts	1
iterated dilated CNN architecture (ID-CNN)	1
character-level ConvNets could work for text classification	1
approximately 2-3%	1
sentences of different languages to the shared-latent space	1
each word	3
relations between them	1
75.2 and 76.1 on validation and test respectively	1
The outputs for each token (e.g., Smith) are twofold: (i) an entity recognition label (e.g., I-PER, denoting the token is inside a named entity of type PER) and (ii) a set of tuples comprising the head tokens of the entity and the types of relations between them (e.g., {(Center, Works for), (Atlanta, Lives in)}).	1
rare words lie in the same region as and are mixed with popular words	1
other two tasks	1
rectified linear units	2
CRF layer	1
Comparing the C-GCN model with the GCN model, we find that the gain mainly comes from improved recall.	1
0.004 and 0.01	1
GeForce GTX TITAN Xp GPU	1
comparable or even superior results	1
structured prediction layer	1
deep CNN (Conneau et al., 2016)	1
different behaviors of the embeddings of popular words and rare words are problematic	1
Open Question Answering	1
Matlab 2012	1
each position separately and identically	1
scientific tasks	1
100 hidden units	1
interactions between sentence pairs	1
IMDB movie review dataset (IMDB)	1
6 layer word-based convolutional networks, with 256/1024 features at each layer, denoted as small/large, respectively	1
CoNLL 2000 Chunking task	1
10x and 20x more data	1
Entity-aware Attention	1
two components	1
higher F1 score	1
1.26%/0.66%/0.44%	1
128	5
similarity used in the embedding space	1
CoNLL test set	1
character-level and subword-level neural machine translation	1
German, Dutch and Spanish	1
8 of 12 language pairs	1
BLSTM	1
a novel class of memory augmented neural networks	1
Experiments	4
extension of the graph convolutional network (Kipf and Welling, 2017; Marcheggiani and Titov, 2017) that is tailored for relation extraction	1
softmax	2
offline	1
DREC dataset	3
NLU	1
AT improves the predictive performance of the baseline model in the joint setting	1
excellent results	1
single maxout (Goodfellow et al., 2013) hidden layer	1
biomedical tasks	1
input sentence	1
LSTM	1
size 50	1
Baseline Systems	1
LSTMs	2
ACE 2005	1
hyperbolic tangent function	1
dissimilar component	1
single	1
{max, global, local-l}	1
BERTSP	1
NMT	2
3 iterations of routing	1
to	1
The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al., 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al., 2017) is a domain independent framework by jointly embedding entity mentions, relation mentions, text features and type labels into meaningful representations.	1
Dutch	1
entity recognition	1
a low-dimensional vector	1
PICO Extraction (PICO)	1
constrained Estonian-English NMT system	1
AWD-LSTM model	1
the number of parameters in the embedding matrix can be huge	1
word representations	1
usual learned linear transformation and softmax function	1
ARC-II outperforms others significantly when the training instances are relatively abundant	1
outperforms the baseline on all datasets	1
DS-logistic (Mintz et al., 2009)	1
a sentence pair	1
new SimpleQuestions dataset	1
zero	1
30% output dropouts	1
novel end-to-end neural model for joint entity and relation extraction	1
for larger datasets	1
other three SWEM variants	1
best performance	1
Text Classification	4
vanilla setting	1
composition functions	1
fast and memory efficient mapping	1
https://github.com/pedrada88/preproc-textclassification	1
extensive experimental investigation	1
WikiText-2 (WT2)	1
we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given; a relation is correct when the type of the relation and the argument entities are both correct	1
Multi-Head Attention	1
each token	1
very deep convolutional network (VD-CNN) (Conneau et al., 2017)	1
dependency tree structures	1
the input	1
memory of the input sequence	1
sigmoid, tanh, and tanh	1
weights of our model	1
{rigid, linear, orthogonal}	1
Matching Natural Language Sentences	1
Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential (left-to-right and right-to-left) and bidirectional tree-structured (bottom-up and top-down) LSTMRNNs.	1
set of tuples comprising the head tokens of the entity and the types of relations between them	1
region embedding + pooling	1
learning	1
neural language modelling	1
convolutional strategy	1
dependency-based sliding window	1
ReLU (Nair and Hinton, 2010) as non-linear activation function	1
each end task	1
in relation classification	1
1024 or 300 units	1
Strict	1
investigating regularization effects of AT in a joint setting for two related tasks	1
parallelized runtime independent of the length of the sequence	1
BC5CDR and ChemProt (Lee et al., 2019)	1
another discriminator	1
self-attentive sentence embeddings	1
our method outperforms the two baselines	1
margin of improvement is more significant	1
decoder reconstructs the sentences	1
match-LSTM model	1
L2 weight decay	1
multiword enhanced vectors	1
traditional methods	1
a massive number of parameters for word embeddings, resulting in a large storage or memory footprint	1
Dropout regularization (Srivastava et al., 2014)	1
regularization method	1
Considering the results in the ACE04, we observe that our model outperforms the model of Katiyar & Cardie (2017) by ~2% in both tasks.	1
>4% improvement	1
weight parameters	1
distant supervised and feature based method	1
multitasking with paraphrase data	1
fixed length sentence encoding vector	1
overall F1 performance (0.4%)	1
including extra information hurts performance	1
all of the three translation tasks	1
larger datasets	1
previous knowledge	1
Relation Classification (REL)	1
one with enough layers to incorporate an effective input width of the same size as that of the dilated network	1
replaces our structured attention layer, and adds indicators of entities (transformed to embeddings) directly to each token's word embedding.	1
element-wise sum of the representations at each word position	1
ARC-I and ARC-II trained purely with random negatives	1
positions from attending to subsequent positions	1
Lample et al., 2017	1
192	1
two sub-layers	2
We observe that our GCN model outperforms all dependency-based models by at least 1.6 F1.	1
strict accuracy	1
catastrophic forgetting	1
neural language model (LM)	1
three different datasets	1
than the other models	1
novel neural attention-based inference model	1
Impact of bidirectionality	1
Based on preliminary tuning, we fixed embedding dimensions nw to 200, np, nd, ne to 25, and dimensions of intermediate layers (nls , nlt of LSTM-RNNs and nhe , nhr of hidden layers) to 100.	1
capsules	1
Recurrent CNN-based model	1
sequential conditional random layer	1
statistical machine translation	1
method	1
a word embedding	1
weight dropout	1
one-layer LSTM	1
bi-directional Long Short Term Memory (Bi-LSTM) layer	1
~2%	2
C-GCN model also outperforms PA-LSTM	1
skip-gram model	1
We employ the technique of early stopping based on the validation set.	1
270K	1
Models	1
BiLSTMs have been shown to suffer several limitations	1
Query-Reduction Network (QRN)	1
It contains a bi-directional Long Short Term Memory (Bi-LSTM) layer to encode the input sentence and a LSTM-based decoding layer with biased loss. The biased loss can enhance the relevance of entity tags.	1
outperform the baselines for 1.06/0.71	1
Sentence Similarity Learning	1
broad biomedical domain	1
comparable results	2
logistic regression model by Rajpurkar et al. (2016)	1
Implementation	2
Corpus	1
TensorFlow (Abadi et al., 2016)	1
proposed model	1
we score an entity as correct if both the entity boundaries and the entity type are correct	1
~3% improvement for both tasks	1
~3%	2
news translation	1
new type of deep contextualized word representation	1
Ablation Studies	1
different combinations and sizes	2
Computer Science	1
to 84.6%	1
beneficial computational and learning properties	1
pre-trained language representation model	2
[1 x 10-3, 3 x 10-4, 2 x 10-4, 1 x 10-5]	1
CNN+LSTM model (best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets)	1
routing by agreement	1
jointly represent both entities and relations with shared parameters in a single model	1
two independent decoders	1
bottoms of the encoder and decoder stacks	1
fixed-length span representations	1
our model is able to outperform both models	1
popular/rare word	1
131072 experts	1
improves every model	1
sentences of length up to 50 word	1
embedding hidden states	1
Cloze task (Taylore, 1953)	1
to 1,000	1
our model performs better than the state-of-the-art model	1
CNN or LSTM models	1
relative usability	1
Model	22
Baseline Models	1
multilingual GNMT model	2
36.3	1
final sentence vector	1
sliding window of context	1
25 dimensional	1
baseline sequence tagger	1
NER, entity linking, and coreference resolution tasks	1
SWEM variants	1
CNN and DAN models are trained	1
N = 10	1
new state-of-the-art performance in terms of MRR	1
accelerated robust subset selection	1
question and answer candidates to the text	1
system of Durrett and Klein (2014)	1
encoding sentences into embedding vectors that specifically target transfer learning	1
recursive neural networks (Socher et al., 2013)	1
Encoder	1
layer below	1
300 hidden units	1
our full model	1
Tree-LSTM (Tai et al., 2015)	1
learning rate ([0:001; 0:015] by step 0:002)	1
3e-4	4
development	1
3e-5	3
significantly degraded	1
Tasks	3
task-specific parameters with word embeddings	1
F1 measure	1
500 maxout units	1
word vectors (i.e., word embeddings)	1
Adam	6
SenMLP	1
CNN-rand	1
learned embeddings of rare words and popular words behave differently	1
compute time	1
every word	1
size 25	1
Methods	1
ABCNN (attention-based CNN)	1
bidirectional LSTM	2
GENIA dataset	1
best method CoType (Ren et al., 2017)	1
other NN models that only use standard word embeddings	1
BERT and the state-of-the-art models	2
QA results	1
14% error reduction	1
two layers	2
{64, 128, 256}	1
a new WordPiece vocabulary on our scientific corpus using the SentencePiece library	1
embedding dimensions	1
dependency structure	1
SimpleQuestions	1
early stopping	5
probability of 0.1	1
1.1 in exact match	1
full 100 billion words	1
tasks other than QA	1
Semi-supervised sequence tagging with bidirectional language models	1
entire dependency tree is present	1
a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity, assuming that the boundaries are known	1
shared parameters	1
parent capsules in the layer above	1
44.77	1
fine-tune embedding-based models	1
Multiple Domains	1
Sparsely-Gated Mixture-of-Experts Layer (MoE)	1
