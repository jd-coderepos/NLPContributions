exception	2
yields reasonably good results	1
associated with	1
optimal	2
above it	1
based on	11
accuracy	2
without	7
setting	1
via	2
such as	12
ran	2
compare with	3
second layer	1
most obvious trend	1
with only	1
even better	1
to predict	4
among all	1
to get	1
contains	7
dilation rates	1
applies	1
telling whether	1
size	1
by optimizing	1
loss-free compression rate	2
extracts	3
aims to detect	2
first layer of the MLP	1
sometimes help	1
lacks reasoning capability	1
achieve	3
significantly outperformed	2
Document-level prediction	1
greatly improves	1
datasets	2
markedly improves	1
to compute	4
after	2
subsuming	1
bit words than	1
minimize	1
sets	1
learn	2
h	1
consisting	1
stronger performances	1
owing to	2
leading to	1
by alleviating	1
decreases	2
mixture of	2
best results	2
isa	1
by leveraging	1
initialize	3
couple	1
up to	1
generates	1
take into account	1
achieves	15
most beneficial	1
got	2
update direction	1
average accuracy	1
improves performance	1
makes it	1
layers	1
most significant difference	1
to achieve	1
to infuse	1
to produce	2
add	1
LSTM layers	1
mini-batch	1
residual blocks	2
obtaining	1
outperformed	7
base learning rate	1
improves the F1-score	1
9 and 8 to	1
parsed	1
fine-tune	4
width and depth	1
applied	1
deploys	1
worked better than	1
representation	1
remove	3
During	1
prevent	2
masked kernel	1
when K = 1	1
at identifying	1
ensuring	1
multiply	1
alleviates	1
strong LSTM and CNN baselines	3
to study	1
perform	3
which require	1
improvement	11
better	1
with	82
modify	1
achieved	9
inject	1
extend with	1
called	36
has description	38
pretraining our word embeddings	1
outperforming	2
state-of-the-art	1
comparable performance	1
30 filters of width 3 characters	1
to prevent	1
tried	1
Stanford Sentiment Treebank (SST-5; Socher et al., 2013)	1
fine-tuned and evaluated	1
basic layers	4
RNNsearch	3
most useful	1
propose	17
do not incorporate	1
naturally host	2
contained up to	1
analyze	2
matches the performance	1
fed into	1
fine tune	1
other	1
does not help	1
layer width of the neural network	1
for binary or fine-grained setting	1
over growing N	1
to pretrain	1
compared to the PA-LSTM	1
indicating	1
indicates	1
contributes very little	1
BERTLARGE outperforms	2
extends filters in	1
Tag "O"	1
jointly pretrains	1
produce	1
compared to	3
removed	5
reducing performance	1
outputs	3
by gaining	1
Size of Mini-Batch	1
prevents	1
implicitly considers	1
rather than	1
exhibiting a better performance	1
grants a bump	1
employed	2
updating	2
optimize	1
implemented	3
implemented using	1
by pruning	1
including	2
more effective	1
Named entity extraction	1
batch size	12
achieves competitive results against	1
SWAG	3
act as	2
exponentially increasing dilation width	1
represented using	1
performs on par with	1
to optimize	1
only used	1
Effect of Model Size	3
were	1
improve	1
to uncover	1
mapped	1
RNNencdec	1
achieve surprisingly good task performance	1
significantly outperforms	6
pre-trained	3
around	2
represents	1
built upon	1
effective	1
help deal with	1
consistently better results	1
design	1
Hyperparamaters	10
chose	1
predict	2
increases the performance	1
performing	1
contributing	1
Size of Hidden Layer	1
despite	1
conduct	2
0.3	1
0.4	2
0.5	1
consists	3
more than an order of magnitude faster	1
learns to construct	1
to address	1
exceeds	1
maintain	1
decay	1
to understand	1
running time	1
dramatic error reduction	1
denoted as	2
builds	1
hurts the result	1
same as those	1
using	32
margin	1
tuning	1
so that	1
substantially improved	1
Code	6
each	3
input	3
to maximize	2
pooling	1
dimensionality l	1
achieves slightly better performance than	1
Machine Translation	2
pretrained on	1
employed on	1
pipelined methods	3
found	4
are	3
contribution of	1
where	6
bias parameter	1
degrades the performance	1
Number of Heads	1
relative to	2
setting of	1
initial learning rate	7
regularize	1
independent of	1
of more than	1
three datasets	3
of about	1
dataset	3
tanh non-linearity	1
through	2
by computing	1
run	2
incorporate	2
contextualizing	1
Sentence-level prediction	4
computed by	1
view	1
only a minor contribution	1
to extract	1
read/write modules	1
has	251
significantly improve	1
given	1
general improvements	1
involves	1
adapt	1
significant speed-up	1
spatially in	1
significant	1
may require	1
applied to	2
decrease	2
converted	1
can query	1
fixed	5
SQuAD v2.0	2
competitive sequence model	1
GloVe word embeddings	1
consists of	12
runs the risk	1
improves F1	1
improved by 4.7%	1
supporting	1
enhance	1
GloVe (Pennington et al., 2014)	1
introduce	9
size of the kernel	1
detects	1
momentum	1
input of each	1
Document Sentiment Analysis	2
as input to	1
takes a input	1
To account for	1
dimensionality	1
Feature-based Approach with BERT	1
higher NER and RC scores	1
stacking layers	1
collected	1
when	9
Training	1
early stopped	1
parameterizing	1
variant of	1
embed	1
as defined in	1
greater	1
utilized	3
first one	1
having	1
For regularization	1
MoE layers	2
to train	3
to improve	1
regularized by	4
provide	1
utilize	2
added to	1
(soft-)searches	2
fed to	1
achieved by	2
to seed	1
Hidden layers	1
operated over	1
takes into account	1
Size of Attention Layer	1
computes	1
Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016)	1
enables	3
activation function	1
means	1
minibatch size	1
explore	3
to allow	1
decomposed	1
to control	1
length penalty	1
learning rates	2
performed	2
presents state-of-the-art (or close to)	1
matches state-of-the-art performances	2
BPTT batch size	1
to supply	1
clip	1
performance of	1
each individual averaged model	1
fed back	1
K = 300	1
transforms	2
replace	2
Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015)	1
used	18
best model	4
to find	1
employs	1
most important conclusion from our experiments	1
consider	4
embedding dimension	1
obtain	4
to stack	1
both	2
important	1
Pre-trained embeddings	1
to initialize	2
operates on	1
Initial Learning Rate	1
hurts	1
leads to	1
effect	1
hidden layer	1
to implicitly capture	1
boundaries evaluation	1
to make	2
significant improvements	1
crucial	1
contribute	1
to validate the hypothesis	1
constructs	1
dimension	2
followed up	1
as a	2
adds	1
Dropout	1
applying	1
split into	1
substituting	1
by sampling from	1
increase	1
to represent	3
differ	1
outperforms	39
to model	3
most important features	1
from the performance gap	1
build	1
in practice	1
uses	12
designed to perform	1
RAM	1
F1-score	2
stack	1
trained on	5
performs better	1
with finetuning	4
fix	1
finds	1
corresponds to	1
evolves	1
in a	2
constructs and labels	1
hidden units	9
Adadelta and stochastic gradient descent	1
trained to minimize	1
higher EC and RC scores	1
Prediction Accuracy	2
in terms of	6
extend	1
to jointly extract	1
as	10
contribute to the effectiveness	1
at	5
switched	1
plays a vital role	1
assigned	1
competitive	1
compare	2
consequently	1
perform favorably	1
by	54
Number of Latent Entity Types	1
F1 drops	1
performs competitively	1
proposed for	1
that ConvNets	1
hidden activations per layer	1
set	4
confirms our hypothesis	1
character embeddings	1
greatly outperforms	1
sample	1
compares	2
better results	1
represent	2
Experimental results	4
essential	1
proposing	1
encode	1
performs	2
RE results	1
added	4
observe	5
average F1	1
perform poorly	1
depending on	1
K increases	1
brings	1
on top of	1
0.05	1
adding	5
window size	2
embeddings of actions	3
train	12
each containing	1
test	3
learns	1
demonstrating the advantage	1
varied	6
updated recurrently	1
conducted using	1
F1-scores	1
average of 0.7%	1
beneficial	1
has value	2
entire length	1
use it in	1
helps	1
carry	1
Cache	1
performs the best	1
L2 Regularization Coefficient	1
MRE in one-pass	4
focus on the role	1
trained	5
optimized	1
implemented with	1
for	90
training	3
conducted between	1
exhibits	1
Semantic role labeling	1
optimizer	2
optimizes	1
alpha	1
averaged	2
highlighting	1
over	12
outperform	3
gain some ability	1
apply dropout	1
similar to	1
MULTILINGUAL MACHINE TRANSLATION	2
effective input width	1
end-to-end tagging models	2
Fixing the vectors	1
affects	1
can be	1
Adam (Kingma and Ba, 2014)	1
beam size	1
dropouts after word embeddings	1
trained with	3
to fix	1
to retain	1
approach the performance	1
size of the masked kernel	1
representation of the source sequence	1
set to	2
avoid	1
training step	1
reaches	2
Answer Sentence Selection	2
may not give us an advantage	1
improves	3
convert	2
construct	2
improved	1
output	1
followed by	2
make use of	1
surpasses	2
classifier	1
model	1
while constructing	1
classifies	1
perform better with	1
tasks	4
reduce	1
test perplexity	2
in	42
hidden dimension	1
constrained and unconstrained NMT	3
lower	1
is	24
encodes	1
stochastic gradient descent	1
removing	3
add 25%	1
provides opportunities	1
Adam optimizer	1
whereby	2
evaluate	6
encoded	1
server	1
to jointly model	1
skips over	1
curve	1
embeddings of POS tags	1
GPUs	2
minimized	1
np, nd, ne	1
warmup_steps	1
treating text	1
LSTM encode/decoders	1
Question answering	1
mask-entity evaluation	1
selected from	2
map	1
within	1
has approach	1
add to	1
dimension of word embedding	1
machine	1
comparable	3
modified version	2
trained by	1
GPU machines	1
Speed Comparisons	2
use	47
implemented in	2
also known as	4
learning rate	14
from sentence	53
LSTM layers in the encoder and decoder	1
while	1
stacked above	1
developed under	1
is used to feed	1
second	2
split	1
Textual entailment	1
trained jointly	1
find	2
than	10
demonstrates the best results	1
significantly outperform	1
selected	2
combine	1
built using	1
number of stacked LSTM layers	1
follows	1
includes	2
achieve best results	1
sequentially processes	1
comparing C-GCN model with the GCN model	1
predicts	1
task	5
incorporating off-path information	1
crop or pad	1
present	1
to preserve	1
code	2
scores	1
operate on	1
conditioned on	1
To further explore	2
nw	1
average variability	1
of	41
designed to be	1
randomly masks	1
on	119
allows	1
improves upon other dependency-based models	1
single relation per pass	1
calculates	1
decomposing and composing	1
evident	1
pre-trained on	4
to decode	1
clipped at	2
concatenate	1
among the best	1
81.1%	1
vary	1
primary advantage	1
initialized by	1
jointly updated	1
decay multiplier	1
slightly better than	1
observed	1
objective	1
CRF layer	1
observes	1
contribution	1
GLUE	3
with-entity evaluation	1
composed of	5
slightly outperforms	2
dropout ratio	2
adversarial training on top of the baseline model	4
two enhancements	4
adopt	3
except	3
receives	1
obtained	7
inspired by	1
access	2
by backpropagating	1
Tag prediction	2
without using	1
increasing further	1
derived from	1
into	5
greatly outperform	1
supports	1
to encode	1
30	1
maintains	2
to determine	1
significantly better results than	2
perform better	2
clearly outperformed	1
makes	1
read from and write to	1
score drops	1
Size of Word Embeddings	1
controlled	1
connected	1
number of hidden units	1
gradient clipping	2
increasing	1
by applying	1
described as	2
to	28
trained using	8
SQuAD v1.1	3
embeddings	1
converting questions	1
pre-trained for	2
treat	1
provides the network	1
jointly extracting methods	3
focuses on	1
lstm units	2
reduce the error	1
most critical component	1
end up	1
to generate	1
input to	2
five	1
to be	1
generate	1
activation functions	1
Coreference resolution	1
baselines	2
maps	2
dropping	1
outperform all systems on all tasks	1
revealing	1
to capture	4
81.4%	1
dropout rate	6
achieving	3
demonstrate	2
suggesting	1
learning character-level word embeddings	1
drop	1
allow	1
to learn	1
first capsule layer	1
update	2
learning	1
Sentence Classification	2
is vital to map	1
observe the importance	1
accomplished using	1
divided into	3
cannot well differentiate	1
is a	9
embedding size	1
every	1
l2 regularizer strength	4
beats the state of the art	1
to stabilize	1
apply	3
beats	1
significantly improved	1
filters	2
to location	1
size of the LSTM	1
do not seem to help	1
Model Architecture	1
significantly improves	2
yields inferior accuracies	1
employ	7
Sentiment analysis	3
with respect to	1
masked	1
tested	2
play an essential role	1
pre-training and fine-tuning	1
during	3
summing up	1
CoNLL 2003 NER task (Sang and Meulder, 2003)	1
selects	1
for the optimization	1
Size of Position Embeddings	1
OntoNotes benchmark (Pradhan et al., 2013)	1
optimization	2
has research problem	159
between	3
state of the art performance	1
does not have	1
word embeddings	7
baseline	3
better separates	1
to compose	1
apply the Ptr-Net model	2
this experiment	1
beam search	1
significantly beats	1
among	4
dropout	3
following	1
by attending to	1
passed through	1
achieved better performance	1
randomly initialize	1
Handling Nested Mentions	1
capsules in the layer below	1
get	3
recall	1
initialized	7
small batch of embeddings	1
slightly better	1
yielding	1
withold	1
further outperforms	1
performs as good or better	1
head number	1
When	2
Results	2
additionally	1
pretrained from	1
Compared with	1
first	2
to remove	1
dimensions	1
mis-classify	1
92.22% F1	1
addresses	1
regularization	1
Natural Language Inference	2
OntoNotes coreference annotations	1
rate of	1
not require	1
for recognizing	1
create	1
from	24
jointly trains	1
weight decay	1
obviously useful	1
trained for	2
two datasets	2
towards	1
sent	1
Adam	2
proposed	1
reusing	1
not statistically significant	1
represent each of the words	1
to rapidly grow	1
before passing	1
substantially and consistently outperforms	1
to a	1
QA results	4
efficiently and explicitly model	1
tune	1
by carefully optimizing	1
hyperparameters	11
considers	1
constructed	1
advantage	1
reads	1
performance	2
at each recurrent step	1
trained base models for	1
to integrate	1
compared with	2
by running	1
attends to	1
additional baseline	1
jumped 3.2%	1
