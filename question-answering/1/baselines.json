{
  "has" : {
    "Baselines" : {
      "is" : {
        "WordEmbed" : {
          "has description" : "represent each short-text as the sum of the embedding of the words it contains. The matching score of two short-texts are calculated with an MLP with the embedding of the two documents as input",
          "from sentence" : "WORDEMBED: We first represent each short-text as the sum of the embedding of the words it contains. The matching score of two short-texts are calculated with an MLP with the embedding of the two documents as input"
        },
        "DeepMatch" : {
          "has description" : "take the matching model in [13] and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer",
          "from sentence" : "DEEPMATCH: We take the matching model in [13] and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer"
        },
        "uRAE+MLP" : {
          "has description" : "use the Unfolding Recursive Autoencoder [19] to get a 100-dimensional vector representation of each sentence, and put an MLP on the top as in WORDEMBED",
          "from sentence" : "uRAE+MLP: We use the Unfolding Recursive Autoencoder [19]3 to get a 100-dimensional vector representation of each sentence, and put an MLP on the top as in WORDEMBED"
        },
        "SENNA+MLP/SIM" : {
          "has description" : "use the SENNA-type sentence model for sentence representation",
          "from sentence" : "SENNA+MLP/SIM: We use the SENNA-type sentence model for sentence representation"
        },
        "SenMLP" : {
          "has description" : "take the whole sentence as input (with word embedding aligned sequentially), and use an MLP to obtain the score of coherence",
          "from sentence" : "SENMLP: We take the whole sentence as input (with word embedding aligned sequentially), and use an MLP to obtain the score of coherence."
        }
      }
    }
  }
}