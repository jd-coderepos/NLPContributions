(Contribution||has||Baselines)
(Baselines||is||WordEmbed)
(WordEmbed||has description||represent each short-text as the sum of the embedding of the words it contains. The matching score of two short-texts are calculated with an MLP with the embedding of the two documents as input)
(Baselines||is||SENNA+MLP/SIM)
(SENNA+MLP/SIM||has description||use the SENNA-type sentence model for sentence representation)
(Baselines||is||SenMLP)
(SenMLP||has description||take the whole sentence as input (with word embedding aligned sequentially), and use an MLP to obtain the score of coherence)
(Baselines||is||uRAE+MLP)
(uRAE+MLP||has description||use the Unfolding Recursive Autoencoder [19] to get a 100-dimensional vector representation of each sentence, and put an MLP on the top as in WORDEMBED)
(Baselines||is||DeepMatch)
(DeepMatch||has description||take the matching model in [13] and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer)
