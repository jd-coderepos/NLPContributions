(Contribution||has||Hyperparameters)
(Hyperparameters||controlled||learning rate)
(learning rate||by||AdaGrad (Duchi et al., 2011))
(AdaGrad (Duchi et al., 2011)||initial learning rate||0.5 (0.1 for QA 10k))
(Hyperparameters||withold||10% of the training)
(10% of the training||for||development)
(Hyperparameters||use||hidden state size)
(hidden state size||of||50)
(Hyperparameters||used||loss function)
(loss function||is||cross entropy between ^v and the one-hot vector of the true answer)
(Hyperparameters||used||Batch sizes)
(Batch sizes||of||128)
(128||for||bAbI QA 10k)
(Batch sizes||of||32)
(32||for||bAbI story-based QA 1k, bAbI dialog and DSTC2 dialog)
(Hyperparameters||used||L2 weight decay)
(L2 weight decay||of||0.001 (0:0005 for QA 10k))
(Hyperparameters||minimized||loss)
(loss||by||stochastic gradient descent)
(stochastic gradient descent||for||maximally 500 epochs)
(maximally 500 epochs||early stopped||loss on development data does not decrease)
(loss on development data does not decrease||for||50 epochs)
