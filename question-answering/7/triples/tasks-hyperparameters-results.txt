(Contribution||has||Tasks)
(Tasks||Answer Sentence Selection||Hyperparameters)
(Hyperparameters||initial learning rate||1e-5)
(Hyperparameters||batch size||4)
(Hyperparameters||word embeddings||pre-trained 300-D Glove 840B vectors)
(pre-trained 300-D Glove 840B vectors||transforms||512-D LSTM inputs)
(512-D LSTM inputs||by||a linear mapping layer)
(Hyperparameters||dropouts after word embeddings||40%)
(Hyperparameters||train||10 epochs)
(Tasks||Answer Sentence Selection||Results)
(Results||exceeds||Our MMA-NSE attention model exceeds the NASM)
(Our MMA-NSE attention model exceeds the NASM||by||approximately 1% on MAP)
(Our MMA-NSE attention model exceeds the NASM||by||0.8% on MRR)
(Tasks||Machine Translation||Hyperparameters)
(Hyperparameters||l2 regularizer strength||3e-5)
(Hyperparameters||regularized by||20% input dropouts)
(Hyperparameters||regularized by||30% output dropouts)
(Hyperparameters||initial learning rate||3e-4)
(3e-4||for||other models)
(Hyperparameters||initial learning rate||1e-3)
(1e-3||for||LSTM-LSTM)
(Hyperparameters||trained to minimize||word-level cross entropy loss)
(Hyperparameters||LSTM encode/decoders||two layers)
(two layers||with||300 units)
(Hyperparameters||batch size||128)
(Hyperparameters||train||40 epochs)
(Tasks||Machine Translation||Results)
(Results||improve||baseline result)
(baseline result||introduce||shared memory access)
(shared memory access||to||encoder-decoder model (NSE-NSE))
(baseline result||by||almost 1.0 BLEU)
(baseline result||replace||LSTM decoder)
(LSTM decoder||with||another NSE)
(Tasks||Natural Language Inference||Hyperparameters)
(Hyperparameters||l2 regularizer strength||3e-5)
(Hyperparameters||initial learning rate||3e-4)
(Hyperparameters||hidden layer||1024 units)
(1024 units||with||ReLU activation)
(1024 units||with||softmax)
(Hyperparameters||batch size||128)
(Hyperparameters||train||40 epochs)
(Tasks||Natural Language Inference||Results)
(Results||obtained||85.4% accuracy score)
(85.4% accuracy score||by||Our MMA-NSE attention model)
(Tasks||Sentence Classification||Hyperparameters)
(Hyperparameters||l2 regularizer strength||3e-5)
(Hyperparameters||second layer||softmax layer)
(Hyperparameters||initial learning rate||3e-4)
(Hyperparameters||for binary or fine-grained setting||1024 or 300 units)
(Hyperparameters||read/write modules||two one-layer LSTM)
(two one-layer LSTM||with||300 hidden units)
(Hyperparameters||batch size||64)
(Hyperparameters||word embeddings||pre-trained 300-D Glove 840B vectors)
(Hyperparameters||first layer of the MLP||ReLU activation)
(Hyperparameters||train||25 epochs)
(Tasks||Sentence Classification||Results)
(Results||outperformed||DMN)
(Results||set||state-of-the-art results on both subtasks)
(Tasks||Document Sentiment Analysis||Hyperparameters)
(Hyperparameters||l2 regularizer strength||1e-5)
(Hyperparameters||stack||NSE or LSTM)
(NSE or LSTM||on top of||another NSE)
(another NSE||for||document modeling)
(Hyperparameters||trained||50 epochs)
(Hyperparameters||trained jointly||The whole network)
(The whole network||by backpropagating||cross entropy loss)
(Hyperparameters||initial learning rate||3e-4)
(Hyperparameters||batch size||32)
(Hyperparameters||used||one-layer LSTM)
(one-layer LSTM||with||100 hidden units)
(100 hidden units||for||read/write modules)
(Hyperparameters||used||pre-trained 100-D Glove 6B vectors)
(Tasks||Document Sentiment Analysis||Results)
(Results||outperformed||Our NSE models outperformed the previous state-of-the-art models)
(Our NSE models outperformed the previous state-of-the-art models||in terms of||both accuracy and MSE)
(both accuracy and MSE||by||approximately 2-3%)
