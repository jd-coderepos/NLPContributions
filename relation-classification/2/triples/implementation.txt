(Contribution||has||Implementation)
(Implementation||using||Python with the TensorFlow machine learning library (Abadi et al., 2016))
(Implementation||from sentence||We have developed our joint model by using Python with the TensorFlow machine learning library (Abadi et al., 2016).)
(Implementation||Training||Adam optimizer (Kingma & Ba, 2015))
(Adam optimizer (Kingma & Ba, 2015)||with||learning rate)
(learning rate||of||10-3)
(Implementation||from sentence||Training is performed using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 10-3.)
(Implementation||size of the LSTM||d=64)
(Implementation||from sentence||We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 (both for the entity and the relation scoring layers).)
(Implementation||layer width of the neural network||l=64)
(Implementation||from sentence||We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 (both for the entity and the relation scoring layers).)
(Implementation||use||dropout)
(dropout||to||regularize our network)
(Implementation||from sentence||We use dropout (Srivastava et al., 2014) to regularize our network.)
(Implementation||has||character-based LSTMs)
(character-based LSTMs||hidden dimension||25)
(Implementation||employ||early stopping)
(Implementation||from sentence||We employ the technique of early stopping based on the validation set.)
(Implementation||obtain||best hyperparameters)
(best hyperparameters||after||60 to 200 epochs)
(Implementation||from sentence||In all the datasets examined in this study, we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset.)
