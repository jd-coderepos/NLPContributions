(Contribution||has||Model)
(Model||consists of||Entity-aware Attention)
(Entity-aware Attention||has description||calculates attention weights with respect to the entity pairs, word positions relative to these pairs, and their latent types obtained by LET)
(Model||consists of||BLSTM)
(BLSTM||has description||sequentially encodes the representations of self attention layer)
(Model||consists of||Self Attention)
(Self Attention||has description||captures the meaning of the correlation between words based on multi-head attention)
(Model||consists of||Word Representation)
(Word Representation||has description||maps each word in a sentence into vector representations)
(Model||is a||novel recurrent neural model)
(novel recurrent neural model||incorporate||entity-aware attention mechanism)
(entity-aware attention mechanism||with||LET method)
