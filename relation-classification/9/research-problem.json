{
  "has research problem" : [
    ["Obtaining large-scale annotated data", 
    {"from sentence" : "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive."}],
    ["address the lack of high-quality, large-scale labeled scientific data",
    {"from sentence" : "We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data."}
    ],
    ["annotated data is difficult and expensive to collect due to the expertise required for quality annotation",
    {"from sentence" : "In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation."}
      ],
    ["unsupervised pretraining of language models on large corpora", 
    {"from sentence" : "As shown through ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks."}],
    ["pretrained language model based on BERT but trained on a large corpus of scientific text",
    {"from sentence" : "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text."}
      ],
      ["pretrained language model based on BERT (Devlin et al., 2019)",
      {"from sentence": "We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data."}]
    ]
}