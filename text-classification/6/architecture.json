{
  "has" : {
    "Architecture" : {
      "has" : {
        "Transformer" : {
          "constructs" : {
            "sentence embeddings" : {
              "using" : "encoding sub-graph of the transformer architecture (Vaswani et al., 2017)"
            },
            "from sentence" : "The transformer based sentence encoding model constructs sentence embeddings using the encoding sub-graph of the transformer architecture (Vaswani et al., 2017)."
          },
          "uses" : {
            "attention" : {
              "to compute" : {
                "context aware representations of words in a sentence" : {
                  "take into account" : "ordering and identity of all the other words"
                }
              }
            },
            "from sentence" : "This sub-graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words."
          },
          "converted" : {
            "context aware word representations" : {
              "to a" : {
                "fixed length sentence encoding vector" : {
                  "by computing" : "element-wise sum of the representations at each word position"
                }
              }
            },
            "from sentence" : "The context aware word representations are converted to a fixed length sentence encoding vector by computing the element-wise sum of the representations at each word position."
          },
          "designed to be" : {
            "general purpose" : {
              "accomplished using" : {
                "multi-task learning" : {
                  "whereby" : {
                    "a single encoding model" : {
                      "is used to feed" : "multiple downstream tasks"
                    }
                  }
                }
              }
            },
            "from sentence" : "The encoding model is designed to be as general purpose as possible. This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks."
          }
        },
        "Deep Averaging Network (DAN)" : {
          "averaged" : {
            "input embeddings for words and bi-grams" : {
              "passed through" : {
                "feedforward deep neural network (DNN)" : {
                  "to produce" : "sentence embeddings"
                }
              }
            },
            "from sentence" : "The second encoding model makes use of a deep averaging network (DAN) (Iyyer et al., 2015) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network (DNN) to produce sentence embeddings."
          },
          "takes a input" : {
            "a lowercased PTB tokenized string" : {
              "outputs" : "512 dimensional sentence embedding"
            },
            "from sentence" : "Similar to the Transformer encoder, the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding."
          },
          "use" : {
            "multitask learning" : {
              "whereby" : {
                "single DAN encoder" : {
                  "to supply" : {
                    "sentence embeddings" : {
                      "for" : "multiple downstream tasks"
                    }
                  }
                }
              }
            },
            "from sentence" : "We make use of multitask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks."
          },
          "primary advantage" : {
            "compute time" : {
              "is" : {
                "linear in length" : {
                  "of" : "input sequence"
                }
              }
            },
            "from sentence" : "The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence."
          }
        }
      }
    }
  }
}