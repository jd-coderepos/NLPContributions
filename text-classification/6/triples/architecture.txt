(Contribution||has||Architecture)
(Architecture||has||Deep Averaging Network (DAN))
(Deep Averaging Network (DAN)||takes a input||a lowercased PTB tokenized string)
(a lowercased PTB tokenized string||outputs||512 dimensional sentence embedding)
(Deep Averaging Network (DAN)||primary advantage||compute time)
(compute time||is||linear in length)
(linear in length||of||input sequence)
(Deep Averaging Network (DAN)||use||multitask learning)
(multitask learning||whereby||single DAN encoder)
(single DAN encoder||to supply||sentence embeddings)
(sentence embeddings||for||multiple downstream tasks)
(Deep Averaging Network (DAN)||averaged||input embeddings for words and bi-grams)
(input embeddings for words and bi-grams||passed through||feedforward deep neural network (DNN))
(feedforward deep neural network (DNN)||to produce||sentence embeddings)
(Architecture||has||Transformer)
(Transformer||converted||context aware word representations)
(context aware word representations||to a||fixed length sentence encoding vector)
(fixed length sentence encoding vector||by computing||element-wise sum of the representations at each word position)
(Transformer||uses||attention)
(attention||to compute||context aware representations of words in a sentence)
(context aware representations of words in a sentence||take into account||ordering and identity of all the other words)
(Transformer||constructs||sentence embeddings)
(sentence embeddings||using||encoding sub-graph of the transformer architecture (Vaswani et al., 2017))
(Transformer||designed to be||general purpose)
(general purpose||accomplished using||multi-task learning)
(multi-task learning||whereby||a single encoding model)
(a single encoding model||is used to feed||multiple downstream tasks)
